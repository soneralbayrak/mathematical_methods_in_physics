\chapter{Definition and fundamentals of Vector Spaces}
\section{Preliminaries: some basic terminology}
\subsection{Primer: groups, fields, and algebras}
The vector spaces are some structures build on fields, which are themselves built on groups. The groups are built on sets. So we start with sets.

Our practical reason to start with sets is that we need to properly understand them to understand vectors, but one can even say that set theory is the basis of all mathematics! Indeed, historically, 20th century Mathematicians did try to build the foundations of math on sets, but Russell's paradox squeezed in and ruined the whole program, at least to the degree it was envisioned back then. Today, even more fundamental approaches (such as category theory or type theory) try to explain whole of math, but this is beyond our scope: sets are good enough to be the foundation of everything we will care in this book, so we will stick with them to prepare the background for vector spaces.

\paragraph{Sets:} A set is a collection of objects, and its elements determine what the set is. Importantly, we know all of the elements of a set!\footnote{This is actually a rather important point for deep mathematical reasons that we will not visit, so what follows in this footnote is beyond the scope of this book (rather disappointing for me).\\\\ The fact that we cannot know everything in a quantum system suggests that we should probably not use sets for quantum information, and this is actually true! The right mathematical object to use in quantum system does not have the type \texttt{Set} but has the type \texttt{Hilb}, or more explicitly, quantum systems belong to a different \emph{category} then the category of sets! An explicit discussion of these would require us to cover category theory, but an interested reader can consult here \draftnote{add sources}.}

Sets can be denoted as explicit list of elements within curly brackets;\footnote{Note that sets are orderless\footnotemark and that duplication of elements do not change a set, e.g.
\bea 
A,B,C::{}&\texttt{Set}\\
A={}&\{1,2,3,4\}\\
B={}&\{1,4,3,2\}\\
C={}&\{1,2,3,4,3,2,1\}
\eea 
implies $A=B=C$.
}
\footnotetext{
Ordered sets are denoted with \emph{round} brackets, e.g.
\bea 
A::{}&\texttt{Ordered Set}\\
A={}&(1,2,3)
\eea 
where $(1,2,3)\ne(1,3,2)$. Ordered sets can be constructed via usual sets, e.g.
\be 
(1,2,3)\leftrightarrow\{\{1\},\{1,2\},\{1,2,3\}\}\\
(1,3,2)\leftrightarrow\{\{1\},\{1,3\},\{1,2,3\}\}\\
\ee 
} for instance
\bea 
A::{}&\texttt{Set}\label{eq: set example type}\\
A={}&\{1,2,3\}\label{eq: set example}
\eea 
Equation~\eqref{eq: set example} shows that the set $A$ consists of the elements $1$, $2$, and $3$. Equation~\eqref{eq: set example type} on the other hand tells us that \emph{$A$ is of type Set}, i.e. $A$ is a set: for more details about this notation, please check out Section~\ref{sec: type notation and functions}.

Sets can have infinitely many elements and we can use dots to infer the rest from the given; for instance, the set $\{\dots,-1,1,3,5,\dots\}$ can denote the set of all odd integers. Although rather convenient, this method is inherently ambiguous; a more proper way to denote sets of infinite elements is through \emph{set comprehensions}.

A set comprehension is a way to build a set, and it is usually used with \emph{predicates}.\footnote{Predicates are functions with the codomain \texttt{Boolean}.\footnotemark In other words, a function is a predicate if it only yields two outputs: \textsf{True}, or \textsf{False}. For instance, the function ``isOddInteger'' defined as
\bea 
\text{isOddInteger}::{}&\texttt{Boolean}\\
\text{isOddInteger}={}&x\to \left(\frac{x+1}{2}\in\Z\right)
\eea 
is a \emph{predicate}, e.g. \mbox{$\text{isOddInteger}(1)=$\textsf{True}}, \mbox{$\text{isOddInteger}(2)=$\textsf{False}}, \mbox{$\text{isOddInteger}(5/2)=$\textsf{False}}, etc.
}
\footnotetext{This is the definition due to \emph{Gottlob Frege}, the father of axiomatic first-order logic. You may find other definitions in different branches of math: an interested reader can check Wikipedia for more.} There are two conventional ways to write down a set $A$ this way:
\begin{itemize}
	\item $A=\{x\in\text{ Domain}\;|\;\text{conditions}\}$
	\item $A=\{f(x)\;|\;\text{conditions}\}$ which is just an abbreviated version of the method above in the sense that this form actually means
\end{itemize}
\be 
A={}&{}\{y\in\text{ Domain inferred from the conditions}\\&\quad|\;\exists x\text{ s.t. }y=f(x)\text{ \& }\text{conditions}\}
\ee 
For instance, the rigorous way to denote the odd integers, compared to a convenient casual notation $\{\dots,-3,-1,1,3,\dots\}$, is\linebreak $\{a\in\Z\;|\;(\exists n\in\Z)\;a=2n+1\}$ by the first approach.\footnote{This would read as \emph{``the set of integer $a$'s for which there exists an integer $n$ such that $a=2n+1$ is true''}.\footnotemark}\footnotetext{
We can understand this more deeply as follows. Consider the \emph{predicate} $P$ defined as 
\be 
P::{}&{}\Z\to\texttt{Boolean}\\
P={}&{}a\to (\exists n\in\Z)\;a=2n+1
\ee 
The set of odd integers are then constructed by using this predicate with the list comprehension, i.e.
\be 
\text{odd integers}=\{a\in\Z\;|\; P(a)\}
\ee 
This also shows how the list comprehension works: $S=\{T\;|\; C\}$ means that the set $S$ consists of all elements of $T$ for which the condition $C$ is true.
}${}^{\;}$\footnote{We remind the reader that $\exists$ reads as \emph{``there exists$\dots$''}, and it is one of the \emph{quantifiers} in logic. The other two important ones that we will make use of are $\forall$ and $\exists !$, which denote \emph{``for all$\dots$''} and \emph{``there exists a unique$\dots$''} respectively. For instance, $(\exists!\;n\in\Z)\;n^2=n$ is the statement \emph{``there exists a unique integer $n$ such that $n^2=n$''}, and it is \textsf{False} as both $0$ and $1$ satisfy $n^2=n$: $\boxed{\left((\exists!\;n\in\Z)\;n^2=n\right)=\textsf{False}}$.\footnotemark\\\\
It is no coincidence that I put the dots to the right of the expressions for the quantifiers; one should read them as in a sentence, hence their order matters. For instance
\bea 
\Big((\forall a\in\Z)(\exists b\in\Z)\;b=2a\Big)=\textsf{True}\\
\Big((\exists b\in\Z)(\forall a\in\Z)\;b=2a\Big)=\textsf{False}
\eea 
where the first expression is \emph{``For all integers, there exists an integer which is twice the former''} and the second one is \emph{``There exists an integer which is twice all integers''}.
}\footnotetext{
It may seem weird at first that there is a variable in the left hand side of the equality $\left((\exists!\;n\in\Z)\;n^2=n\right)=\textsf{False}$, which does not appear on the right hand side. The variable $n$ here is a \emph{dummy/scooping/bound} variable hence the right hand side should be independent of it, just like the function $g(y)$ in $\int f(x,y)dx=g(y)$ being independent of $x$. For more details about this, see the comprehensive footnote~\ref{footnote:dummy variables}.} In comparison, the second approach makes it simpler: $\{2n+1\;|\; n\in\Z\}$.

I will assume that the readers are familiar with the comparisons of sets\footnote{$A\subseteq B$ (or $B\supseteq A$) means that all elements of $A$ are also elements of $B$ ($A$ is called a \emph{subset} of $B$), and $A\subset B$ (or $B\supset A$) means the same thing with the additional condition that $A\ne B$ ($A$ is called a \emph{proper subset} of $B$).} and miscellaneous details of sets.\footnote{For instance, $\abs{S}$ denotes the number of elements in the set $S$, e.g. $\abs{\{1,2,4\}}=3$. If the set $S$ is infinite, we write $\abs{S}=\infty$. As expected, $S\subseteq T$ implies $\abs{S}\le\abs{T}$. I would also like to introduce \emph{power sets}: the power set of a set $S$ is denoted by $\mathcal{P}(S)$ and it is the set whose elements are the subsets of the set $S$. For example,
\bea
S,\mathcal{P}(S)::{}&{}\texttt{Set}\\
S={}&{}\{a,b,1\}\\
\mathcal{P}(S)={}&{}\Big\{\{\},\{a\},\{b\},\{1\},\{a,b\}\\
{}&{}\{a,1\},\{b,1\},\{a,b,1\}\Big\}
\eea 
We note that if the set $S$ is finite, we have $\abs{\cP(S)}=2^{\abs{S}}$, which is indeed the case in the above example ($8=2^3$). This relation is why the power set of $S$ is also loosely denoted as $2^S$.
} I will also skip discussing the details of operations between sets.\footnote{Six important operations to know are \emph{union}, \emph{intersection}, \emph{difference} (also called \emph{complement}), \emph{Cartesian product}, \emph{disjoint union}, and \emph{quotient by an equivalence relation}. I expect the reader to know the first four operations: the last two will not be relevant for this book, but I advice the interested reader to learn more about them.} Lastly, I would like to mention the relevant concepts such as \emph{relations}, \emph{partitions}, and \emph{quotients}: one should know about them for a good general mathematical background, but they will unfortunately not be covered in this book.

\paragraph{Groups:} Consider a set $S$ and impose the existence of two functions $o$ and $i$ such that 
\bea 
S::{}&{}\texttt{Set}\\
o::{}&{}(S,S)\to S\\
i::{}&{}S\to S
\eea 
where $S$ is of type set, and the function $o$ has the domain ``tuple of $S$'' and the codomain $S$ ---check out section~\ref{sec: partial differential equations} for more details about tuples. In simpler terms, the function $o$ takes two inputs (both being elements of the set $S$) and yields an element of the set $S$; in comparison, the function $i$ takes an element of $S$ to another (not necessarily different) element of $S$.

The set $S$ with these two functions $o$ and $i$ form a group if the following statements are true:
\begin{enumerate}
	\item $\boxed{(\exists e\in S)(\forall s\in S)\; o(e,s)=o(s,e)=s}$. In words, this says that\linebreak \emph{``There exists an element $e$ of the set $S$ and $o(e,s)=o(s,e)=e$ for any element $s$ of this set''}. The element $e$ is called \emph{the identity element} with respect to the \emph{group operation} $o$.\footnote{Suppose for a second that there are two identity elements: $e_1,e_2$. Since $e_1$ is identity element, we need $o(e_1,e_2)=e_1$; but the same argument for $e_2$ dictates $o(e_1,e_2)=e_2$: we need $e_1=e_2$. In conclusion, if identity element exists, it is \emph{unique}!}
	\item  $\boxed{(\forall s \in S)\; o(s,i(s))=o(i(s),s)=e}$. In words, this says that \emph{``The group operation $o$ acting on the tuple $(s,i(s))$ or $(i(s),s)$ yields the identity element $e$, for any element $s$ of the set $S$''}. In practice, this condition specifies the action of $i$ on any element $s$, and $i(s)$ is called the \emph{inverse} of the element $s$.
	\item $\boxed{(\forall a,b,c \in S)\; o(a,o(b,c))=o(o(a,b),c)}$. This is called \emph{associative law}, and it is best visualized when we use \emph{infix notation}: if we denote $o(a,b)$ as $a*b$, then this condition simply reads as $a*(b*c)=(a*b)*c$. Therefore, the parentheses are not important, and we can even loosely denote without parentheses: $a*(b*c)=(a*b)*c=a*b*c$.
\end{enumerate}

We will \emph{define} a group to be the pair $(S,o)$, i.e.
\be 
\texttt{Group}=\Big(\texttt{Set},\;(\texttt{Set},\texttt{Set})\to\texttt{Set}\Big)
\ee 
such that the above statements are true (satisfaction of those rules will in turn determine the inverse function $i::S\to S$ as well). For instance, the pair $(\Z,+)$ is a group, where $+$ denotes the arithmetic addition, and the inverse function turns out to be multiplication by $-1$: we say that \emph{integers form a group under arithmetic addition}. We can indeed check that the above statements are true:
\begin{enumerate}
	\item $(\forall a\in \Z)\; a+0=0+a=a$ (addition by $0$ does not change any integer)
	\item $(\forall a\in \Z)\; a+(-a)=(-a)+a=0$ (addition by the inverse element takes an integer to zero)
	\item $(\forall a,b,c \in \Z)\; a+(b+c)=(a+b)+c$ (the order of addition does not change the result)
\end{enumerate}

As another example for groups, consider the set\linebreak$\textsf{classrooms}=\{P1,P2,P3\}$ and the group operation denoted as $<>$ in infix notation. If we are given the information
\be 
\label{ex: classrooms}
P1<>P1={}&{}P3\\
P2<>P2={}&{}P2\\
P3<>P3={}&{}P1\\
P1<>P2=P2<>P1={}&{}P1\\
P1<>P3=P3<>P1={}&{}P2\\
P2<>P3=P3<>P2={}&{}P3
\ee 
we can immediately deduce that $P2$ is the identity element, and $P1^{-1}=P3$, $P2^{-1}=P2$, \& $P3^{-1}=P1$ (inverse operations). One can check that the necessary conditions are satisfied, hence we state
\be 
(\textsf{classrooms},<>)::\texttt{Group}
\ee 
We will leave further details of groups to the interested reader to research! One important exception will be the concept of \emph{commutativity}: if the group operation is commutative, i.e. $a*b=b*a$, then that group is called a \emph{commutative group}. In commutative groups, one conventionally use the infix operator $+$ instead of $*$ for the group operation, $-s$ instead of $s^{-1}$ to denote inverse operation, and $0$ instead of $1$ to denote identity element. For instance, we can rewrite the example in \equref{ex: classrooms} as $\textsf{classrooms}=\{P1,P2,P3\}=\{P1,0,-P1\}$. Note that the minus ($-$) sign and the symbol $(0)$ has nothing to do with the arithmetic negation and the number $0$ in general: this is simply a conventional notation.

\paragraph{Rings:} Consider a set $S$ and two functions $+$ and $\.$ such that 
\bea 
S::{}&{}\texttt{Set}\\
+::{}&{}(S,S)\to S\\
\.::{}&{}(S,S)\to S
\eea 
where we will denote the functions in the infix form, e.g. $a+b$ instead of $+(a,b)$. We say that the triplet $(S,+,\.)$ is a ring, i.e.
\be 
(S,+,\.)::\texttt{Ring}
\ee
if the following statements are true:
\begin{enumerate}
	\item $\boxed{(S,+)\text{ is a commutative group}}$
	\item $\boxed{(\forall a,b,c\in S)\; a\.(b+c)=a\.b+a\.c}$
	\item $\boxed{(\forall a,b,c\in S)\; (b+c)\.a=b\.a+c\.a}$
\end{enumerate}
In other words, rings are simply commutative groups with an additional operation (call multiplication) that distributes over group addition.

The most familiar rings are those where the ring addition and multiplication operations are literally arithmetic addition and multiplication; for instance, $(\Z,+,\.)$ forms a ring and so does $(\R,+,\.)$. Likewise, rectangular matrices with complex entries form a ring under matrix addition and matrix multiplication, i.e. $(\cM_{n\x n}(\C),+,\.)::\texttt{Ring}$.

\paragraph{Skew field:} We say that a triplet $(S,+,\.)$ is a skew field, i.e.
\be 
(S,+,\.)::\texttt{Skew Field}
\ee
if the following statements are true:
\begin{enumerate}
	\item $\boxed{(S,+,\.)\text{ is a ring}}$
	\item $\boxed{(S\backslash\{0\},\.)\text{ is a group}}$
\end{enumerate}
In other words, skew fields are rings where multiplication operation also forms a group if we remove the identity element of the addition operation. The prototypical example of the skew fields is the \emph{quaternions}.\footnote{\draftnote{Put some discussion of quaternions}.}

\paragraph{Fields:}  We say that a triplet $(S,+,\.)$ is a field, i.e.
\be 
(S,+,\.)::\texttt{Field}
\ee
if the following statements are true:
\begin{enumerate}
	\item $\boxed{(S,+,\.)\text{ is a ring}}$
	\item $\boxed{(S\backslash\{0\},\.)\text{ is a commutative group}}$
\end{enumerate}

Fields are ubiquitous and super important for physicists as we use them everwhere! The rational, real, and complex numbers all form fields under ordinary arithmetic addition and multiplication:\footnote{\label{footnote: exotic example for a field}
For a more exotic example, consider the following set:
\be 
A=\Big\{(x\to x+a)\;\Big|\; a\in \R\Big\}
\ee 
for the ordinary arithmetic addition. This is a set of \emph{functions}, i.e. elements of the set are functions; for instance, one of the elements of this set is $x\to x+1$: this is a function that adds one to its input.
\\\\
Let us rewrite this set in a more familiar way. For this, we define the \emph{higher order function} ``\textsf{plus}'':
\be 
\textsf{plus}::{}&{}\R\to(\R\to\R)\\
\textsf{plus}={}&{}a\to\big(x\to(x+a)\big)
\ee 
This higher order function becomes an ordinary function if given an input, i.e. $\textsf{plus}(1)::\R\to\R$, which can then be given one more input that converts it into a real number: $\textsf{plus}(1)(5)=6::\R$. Indeed, this higher order function simply adds its inputs, i.e. $\textsf{plus}(a)(b)=a+b$. In terms of this higher order function, we can write down the set $A$ above as
\be 
A=\Big\{\textsf{plus}(a)\;\Big|\; a\in \R\Big\}
\ee 
Let us now introduce two higher order functions:
\be
\oplus,\otimes ::{}\big((\R\to\R),(\R\to\R)\big)\to (\R\to\R)
\ee
These functions take a pair of functions, and then give another function! Let us use these guys in infix form, e.g. $\textsf{plus}(a) \oplus\textsf{plus}(b)$.
\\\\
If we are given the information that 
\be 
\textsf{plus}(a) \oplus\textsf{plus}(b)=\textsf{plus}(a+b)
\ee 
we can immediately say $(A,\oplus)$ forms a commutative group, with the identity element $\textsf{plus}(0)$. If we are given the further information
\bea 
{}&\big(\textsf{plus}(a) \oplus\textsf{plus}(b)\big) \otimes\textsf{plus}(c)=\nn\\
{}&\big(\textsf{plus}(a)\otimes\textsf{plus}(c)\big) \oplus\big(\textsf{plus}(b)\otimes\textsf{plus}(c)\big)
\\
{}& \textsf{plus}(c)\otimes\big(\textsf{plus}(a) \oplus\textsf{plus}(b)\big)=\nn\\
{}&\big(\textsf{plus}(c)\otimes\textsf{plus}(a)\big) \oplus\big(\textsf{plus}(c)\otimes\textsf{plus}(b)\big)
\eea 
we can then say that $(A,\oplus,\otimes)$ forms a ring! Finally, if we are also given
\be 
\textsf{plus}(a) \otimes\textsf{plus}(b)=\textsf{plus}(a.b)
\ee 
then we can prove that $(A,\oplus,\otimes)$ is actually a field!
}
\be 
(\Q,+,\.)::\texttt{Field}\;,\qquad (\R,+,\.)::\texttt{Field}\;,\qquad (\C,+,\.)::\texttt{Field}
\ee  

\paragraph{Vector space (Linear space):} We \emph{define} \emph{``a vector space over a field $F$''} as the triple $(V,\oplus,\odot)$, i.e.
\bea
V::{}&{}\texttt{Set}\\
\Big(F=(S,+,\.)\Big)::{}&{}\texttt{Field}\\
\oplus::{}&{}(V,V)\to V\\
\odot::{}&{}(S,V)\to V\\
(V,\oplus,\odot)::{}&{}\texttt{Vector Space}
\eea
if the following statements are true
\begin{enumerate}
	\item $\boxed{(V,\oplus)::\text{Commutative Group}}$
	\item $\boxed{(\forall v\in V)\; 1\odot v=v\text{ where $1$ is the identity element of $\.$}}$
	\item $\boxed{(\forall v\in V)(\forall s\in S)\; s\odot v \in V}$
	\item $\boxed{(\forall v\in V)(\forall a,b\in S)\; (a\.b)\odot v=a\odot (b\odot v)}$
	\item $\boxed{(\forall v\in V)(\forall a,b\in S)\; (a+b)\odot v=(a\odot v)\oplus (b \odot v)}$
	\item $\boxed{(\forall v,w\in V)(\forall s\in S)\; s\odot(v\oplus w)=(s\odot v)\oplus (s\odot w)}$
\end{enumerate}
Here, the elements of the underlying field $F$ are called \emph{scalars} ($s\in S$) and the elements of the linear spaces are called \emph{vectors} ($v\in V$). indeed, $V$ above is the set of these vectors, $\oplus$ is the \emph{vector addition} operation, and $\odot$ is the \emph{scalar multiplication} operation. We will see more properties of vectors (such as basis, linear independence, etc.) in the next section.

The standard example of a linear space is this:
\be
\big(
\left(\cM_{n\x m}(S),\oplus\right),\left(S,+,\.\right)
\big)::\text{Linear Space}
\ee 
In words, we say that all $m\x n$ matrices from a field $\ka=\left(S,+,\.\right)$ form a linear space over $\ka$. In this example, matrices \emph{are} vectors of this linear space and the vector addition $\oplus$ is simply defined as the matrix addition. A more familiar example can be given as follows:
\be 
V={}&{}\{a\bx+b\by+c\bz \;|\; a,b,c\in\R\}\\
\left(a_1\bx+b_1\by+c_1\bz\right)\oplus\left(a_1\bx+b_1\by+c_1\bz\right)={}&{}(a_1+a_2)\bx+(b_1+b_2)\by+(c_1+c_2)\bz\\
s\odot\left(a\bx+b\by+c\bz\right)={}&{}(s\.a)\bx+(s\.b)\by+(s\.c)\bz
\ee 
where we defined the set $V$ and the explicit action of the functions $\oplus$ and $\odot$.\footnote{
Let us consider a more nontrivial example. Consider the field $(A,\oplus,\otimes)$ disucssed in footnote~\ref{footnote: exotic example for a field}. We will define a vector space $(W,\square,\odot)$ over this field. Note that, in this example, $\square$ will denote the vector addition whereas $\oplus$ is reserved for the scalar addition in the field $A$.
\\\\
We define the set $W$ as
\be 
W=\{(a,b)\;|\;a,b\in\R\}
\ee  
whereas we choose the vector addition and scalar multiplications as
\be 
(a,b)\square (c,d)=(a+c,\;b+d)
\ee 
and
\be 
\textsf{plus}(a)\odot (b,c)=(a\.b,\;a\.c)
\ee 
With these definitions, we can verify that all conditions for $(W,\square,\odot)$ to be a vector space over the field $A$ are satisfied.
}
\draftnote{
\begin{enumerate}
	\item (Linear algebra$=$linear space with vector multiplication) is introduced and discussed.
	\item (Lie algebras$=$linear algebra with multiplication being antisymmetric) is introduced and discussed.
	\item Made reference to the commutators and discussed how a linear algebra can be turned into a lie algebra via commutators.
	\item Cross products in three dimensional vector space is discussed from this perspective: $3d$ vectors form a lie algebra thanks to the cross product.
	\item Emphasized how vector spaces can be built on an arbitrary set $V$ as long as the necessary conditions are satisfied. For instance, we can choose
	\be 
	V=\Big\{a\pdr{}{x}+b\pdr{}{y}+c\pdr{}{z}\;\Big|\;a,b,c\in\R\Big\}
	\ee 
	With arithmatic addition and multiplication, these indeed form a vector space. In fact, this way of definition is far more useful than $a\bi+b\bj+c\bk$ as we will see later.
\end{enumerate}
}

\subsection{Properties of Linear spaces}
\draftnote{
\begin{enumerate}
	\item Basis for vector spaces is introduced: let $(B\supset V)::\texttt{Set}$. $B$ is called \emph{a basis} of $V$ if following statements are true:
\begin{itemize}
	\item $(\forall k\in\{1,2,\dim B\})(\forall e_1,\dots,e_k\in B)(\forall c_1,\dots,c_k\in S) [c_1=\dots=c_k=0]\lor[c_1e_1+\dots+c_ke_k\ne 0]$
	\item $(\forall v\in V)(\exists ! a_1,\dots,a_{\dim B}\in S) v=a_1e_1+\dots a_{\dim B}e_{\dim B}$
\end{itemize}
The first statement is the \emph{linear independence} of the basis vectors, whereas second statement is the requirement that the basis \emph{spans} the vector space, i.e. any vector can be decomposed in terms of the basis elements. The coefficients $a_i$ are called the \emph{components} of the vector in this basis.
\item If $\dim B<\infty$, it is called the dimension of the vector space!
\item If $\dim B=\infty$, the vector space is called \emph{infinite dimensional}!
\item Using a basis, a vector can be related to a list/array/tuple of scalars:
\be 
\left.\begin{aligned}
	v::&\texttt{Vector}\\
	a_1,\dots,a_n::&\texttt{Scalar}
\end{aligned}\right\}\Rightarrow v=\begin{pmatrix}
	a_1&\dots&a_n
\end{pmatrix}\begin{pmatrix}
	e_1\\\dots\\e_n
\end{pmatrix}
\ee 
For instance, $(3\bi+4\bk)::\texttt{Vector}$ whereas $(3,0,4)::(\texttt{Scalar},\texttt{Scalar},\texttt{Scalar})$; nevertheless, they represent the same information once the proper basis is given: $3\bi+4\bk=\begin{pmatrix}
	3&0&4
\end{pmatrix}\begin{pmatrix}
	\bi\\\bj\\bk
\end{pmatrix}$.
\item If multiple basis exist for a vector space, all have the same dimension and all can be converted to one another (proof omitted).
\item Index notation and Einstein's summation convention is discussed, i.e. $v=v^ie_i$. Note that index placement is important, i.e. $e^i\ne e_i$.
\item One can choose $a_i$ to represent a row or column matrix (and $a^i$ the other one). Note that $M_i^{\;j}$ or $M^i_{\; j}$ represent a square matrix (as they convert a row/column matrix to another row/column matrix), but $M_{ij}$ or $M^{ij}$ are not square matrices.
\end{enumerate}
}

\section{Inner product spaces}
\draftnote{
\begin{enumerate}
\item Normed vector spaces: in general, we do not need to have functions that takes vectors as inputs and spits out scalars as outputs, i.e. no $(\cdot)::(V,V)\to S$ or $\norm{.}::V\to S$ is necessary for the definition of the vector space.
\\\\
If we impose that a function \emph{\textsf{norm}} such that
\be 
\textsf{norm}::&V\to\R \\
\textsf{norm}=& x\to\norm{x}
\ee 
exists where
\begin{itemize}
	\item $(\forall v\in V) [\norm{v}\ne 0]\lor[v=0]$
	\item $(\forall v\in V)(\forall s\in S)\norm{s\odot v}=\abs{s}.\norm{v}$
	\item $(\forall v,w\in V)\norm{v\oplus w}\le\norm{v}+\norm{w}$
\end{itemize}
and where $V$ is a vector space over the field $\R$ or $\C$, then we call $(V,\norm{.})$ a \emph{normed vector space}.
\item With norm, we can define distance between two vectors: $d::(V,V)\to\R$ and $d=(x,y)\to\norm{x-y}$.
\item With norm, we can define angle between two vectors: $\theta::(V,V)\to\R$ and $\displaystyle \theta=(x,y)\to\acos(\frac{\norm{x}^2+\norm{y}^2-\norm{x-y}^2}{2\norm{x}\norm{y}})$.
\item Inner product spaces: If we have a vector space over the field $\R$ or $\C$, and if we impose the existence of the function $\<.,\.\>$ for 
\be 
F\in&\{\R,\C\}\\
V::&\texttt{Vector Space over }F\\
\<.,.\>::&(V,V)\to F
\ee 
where the following are true:
\begin{itemize}
	\item $(\forall v,w\in V)\;\<v,w\>=\<w,v\>^*$ (conjugate symmetry)
	\item $(\forall u,v,w\in V)(\forall a,b\in F)\; \<au+bv,w\>=a\<u,w\>+b\<v,w\>$
	\item $(\forall v\in V\backslash\{0\})\;\<v,v\>>0$
	\item $\<0,0\>=0$
\end{itemize}
\item Inner product vector spaces can immediately be turned to a normed vector space as well as we can  define $\norm{v}=\sqrt{\<v,v\>}$ which is consistent with all norm requirements. In this definition, the angle between two vectors become $\displaystyle\cos\theta=\frac{\Re(\<x,y\>)}{\norm{x}\norm{y}}$.
\end{enumerate}
}
\section{Dual vector spaces}
\draftnote{
\begin{enumerate}
\item Dual vector spaces: Let $V$ be a vector space over a field $F$. If we consider linear functions that take a vector to a scalar, those functions themselves form a vector space: that is called the \emph{dual vector space} $V^*$. So we have 
\bea 
(F=(S,+,.))::&\texttt{Field}\\
V::&\texttt{Set}\\
\oplus::&(V,V)\to V\\
\odot::&(S,V)\to V\\
(V^*=\{V\to S\})::&\texttt{Set}\\
\oplus^*::&(V^*,V^*)\to V^*\\
\odot^*::&(S^*,V^*)\to V^*\\
\eea 
where 
\bea 
(V,\oplus,\odot)=\text{ vector space over }F\\
(V^*,\oplus^*,\odot^*)=\text{ vector space over }F
\eea 
Gave several examples in class, for instance $V=\{a\bi+b\bj\;|\;a,b\in\R\}$ and $V^*=\{(x\bi+y\bj)\to(ax+by)\;|\;a,b\in\R\}$.
\item Conventionally, elements of $V$ are called \emph{vectors} and elements of $V^*$ are called \emph{covectors}.
\item For the vector space given in the notation
\be 
\label{eq: vector derivative notation}
V=\Big\{a\pdr{}{x}+b\pdr{}{y}+c\pdr{}{z}\;\Big|\;a,b,c,\in\R\Big\}
\ee 
the dual vector space is denoted as
\be 
V^*=\big\{adx+bdy+cdz\;\big|\;a,b,c\in\R\}
\ee 
where $dx\left(\pdr{}{x}\right)=1$, $dx\left(\pdr{}{y}\right)=0$, and similarly.

Note that this convention is more than just a notation: the vector spaces indeed transform functions to functions, and integrations are indeed summations of covectors in a loose sense. We will make these much more concrete later.
\item Discussed in detail and derived that if a basis of the vector space $e_i$ and the basis of the dual vector space $e^i$ transform as 
\be 
e_i\to& M_i^{\;k}e_k\\
e^i\to& e^kN_k^{\;\;i}
\ee 
then the components of the basis vector $v^i$ and the components of the covector $w_i$ transform as 
\be 
v^i\to& v^k(M^{-1})_k^{\;\;i}\\
w_i\to& (N^{-1})_i^{\;k}w_k
\ee 
If we want the action of the covector basis on vector basis to remain unchanged, i.e.
\be 
e^i(e_k)\to e^i(e_k)
\ee 
then we need $N=M^{-1}$. With this, we see that
\be 
e_i\to& M_i^{\;k}e_k\\
w_i\to& M_i^{\;k}w_k\\
e^i\to& e^k(M^{-1})_k^{\;\;i}\\
v^i\to& v^k(M^{-1})_k^{\;\;i}
\ee
We see that the array $w_i$ transforms in the same way as the basis vectors of $V$, i.e. it transforms \emph{covariantly}. We then loosely state that $w_i$ is a \emph{covariant vector}!\footnote{Note that $v^i$ ($w_i$) is actually only the components of the (co)vector, hence it is only an array of the underlying field , i.e. $w_i::(S,S,\dots,S)$ and $v^i::(S,S,\dots,S)$. In contrast, the combination $(w_ie^i)::V^*$ is a covector and the combination $(v^ie_i)::V$ is a vector.} In contrast, $v^i$ transforms in an inverse way compared to $e_i$, and hence it is loosely called a \emph{contravariant vector}!
\end{enumerate}
}
\draftnote{We did not discuss topological vector spaces and Hilbert spaces this year: we may discuss it next year!}

\section{Algebras over fields}
\subsection{Tensor algebras}
\draftnote{
\begin{enumerate}
	\item Consider a linear space over a field $F$. We call \emph{``type $(r,s)$ tensor on a vector space $V$''} an element of 
	\be 
	\underbrace{V\otimes V\otimes\cdot\otimes V}_{r}\otimes\underbrace{V^*\otimes V^*\otimes\cdot\otimes V^*}_{s}
	\ee 
	where $\otimes$ is an associative bilinear map, i.e.
	\be 
	&A=\{V,V^*\}\\
	&\otimes::(A,A)\to A\otimes A\\
	&(\forall a,b,c\in A)\;(a\otimes b)\otimes c=a\otimes(b\otimes c)
	\ee
\item We conventionally denote
\be 
T^kV=\underbrace{V\otimes\cdots\otimes V}_{k}
\ee 
Since the bilinear map $\otimes$ takes the product of $v:: T^kV$ and $\w:: T^l V$ to $(v\otimes\w)::T^{k+l}V$, we need the consider the space of all possible $T^kV$ to have an algebra: this is denoted as $T(V)$ (tensor algebra over $V$) and is defined as
\be 
T(V)=\bigoplus\limits_{k=0}^\infty T^kV=F\oplus V\oplus(V\otimes V)\oplus\cdots
\ee 
A similar definition can be made for $T(V^*)$; the product $T(V)\otimes T(V^*)$ is the algebra over vector field that includes all type of tensors (i.e. $(\forall r,s\in\N)\;(r,s)-$tensor).
\item In Physics, we usually work with components. Indeed, consider the following example:
\be 
a::&V\otimes V\otimes V^*\\
a=&a^{ij}_{\;\;k}e_i\otimes e_j\otimes e^k
\ee 
We usually call $a^{ij}_{\;\;k}$ a tensor: in reality, it is only a collection of scalars (underlying field elements of the vector space); nevertheless, once a basis is specified, there is indeed a unique map between $a$ and $a^{ij}_{\;\;k}$ hence this abuse of notation is justified.
\item $\otimes$ provides an \emph{outer} product, i.e. $a\otimes b$ is not part of the types of $a$ or $b$. For instance, if both $a$ and $b$ are of the type $(1,0)$, $a\otimes b$ is of the type $(2,0)$. In general the value $r+s$ for a type $(r,s)$ tensor is called \emph{the rank of the tensor}, and the rank of the tensor $a\otimes b$ is the sum of the ranks of the tensors $a$ and $b$. For instance:
\be 
\label{eqn: tensor example}
a::&V\otimes V\otimes V^*\\
b::&V^*\otimes V^*\\
(c=a\otimes b)::&V\otimes V\otimes V^*\otimes V^*\otimes V^*\\
a=&a^{ij}_{\;\;k}e_i\otimes e_j\otimes e^k\\
b=&b_{lm}e^l\otimes e^m\\
c=&c^{ij}_{\;\;klm}e_i\otimes e_j\otimes e^k\otimes e^l\otimes e^m\\
c^{ij}_{\;\;klm}=&a^{ij}_{\;\;k}b_{lm}
\ee 
\item As stated earlier, we usually work with components in practice (at least in the Physics), so the product of two tensors $a^{i_1\dots i_r}_{\quad\;\; k_1\dots k_s}$ and $b^{l_1\dots l_t}_{\quad\;\; m_1\dots m_u}$ just becomes $c^{i_1\dots i_r l_1\dots l_t}_{\qquad\quad k_1\dots k_s m_1\dots m_u}=a^{i_1\dots i_r}_{\quad\;\; k_1\dots k_s}b^{l_1\dots l_t}_{\quad\;\; m_1\dots m_u}$.
\item There is a way to change the type and rank of a tensor by an operation called \emph{contraction}. What it does is basically act the basis covector on the basis vector; to see this, consider the tensor $c$ in the equation~\ref{eqn: tensor example}. We can \emph{choose} to contract some of its basis elements to reduce its rank, for instance if we choose to act $e^k$ on $e_i$, we get $e^k(e_i)=\de^k_{\;i}$ for the kronecker-delta $\de$. But since $c^{ij}_{\;\;klm}\de^k_{\;i}=c^{ij}_{\;\;ilm}$, we arrive at
\be 
c'::&V\otimes V^*\otimes V^*\\
c'=&c^{ij}_{\;\;ilm} e_j\otimes e^l\otimes e^m
\ee 
If we instead acted $e^k$ on $e_j$, we would get a different tensor $c''$:
\be 
c''::&V\otimes V^*\otimes V^*\\
c''=&c^{ij}_{\;\;jlm} e_i\otimes e^l\otimes e^m
\ee
Or we could act $e^k$ on $e_i$ and $e^l$ on $e_j$: this would give us an entirely different tensor $c'''$:
\be 
c'''::& V^*\\
c'''=&c^{ij}_{\;\;ijm}e^m
\ee
\item Consider the rotational version of Newton's second law, i.e. \emph{``torque is equal to moment of inertial times angular acceleration''}. We conventionally write it as a matrix equation, i.e. $\tau^i=I^i_{\;j}\a^j$. We see that we could write them as follows:
\be 
\tau,\alpha::&V\\
I::&V\otimes V^*\\
(I\otimes \alpha)=& I^i_{\;j}\alpha^k e_i\otimes e_k\otimes e^j\\
\tau=&I^i_{\;j}\a^j e_i
\ee 
Clearly, we got $\tau$ by first taking the outer product of $I$ and $\a$, and then doing a contraction between two of the indices (equivalently, applying the covector $e^j$ on the vector $e_k$).\footnote{
This line of reasoning is valid to a degree for $3d-$vectors, but in higher dimensions, it is not true. Firstly, $\tau$ and $\a$ are not vectors but \emph{bivectors} in general dimension (we'll cover bivectors below), hence $I$ is actually not a matrix (a rank-2 tensor) but a linear map of bivectors to bivectors (hence a rank-4 tensor): $I::V\otimes V\otimes V^*\otimes V^*$.
}
\end{enumerate}
}
\subsection{Exterior algebras}
\draftnote{\begin{enumerate}
\item Tensor product $\otimes$ is only defined as an associative bilinear mapping. If we define a similar product with the additional property that it is antisymmetric, we get \emph{exterior algebra}.
\item The relevant antisymmetric operator is called \emph{Wedge product} and is denoted as $\wedge$:
\begin{itemize}
	\item $(\forall u,v\in V)\; u\wedge v=-v\wedge u$
	\item $(\forall u,v,w\in V)\; u\wedge(v\wedge w)=(u\wedge v)\wedge w$
\end{itemize}
\item As the product is fully antisymmetric, any wedge product of $k-$many vectors is zero if $k>\dim(V)$, thus:
\be 
\Lambda(V)=\bigoplus\limits_{k=0}^{\dim(V)} \Lambda^kV=F\oplus V\oplus(V\wedge V)\oplus\cdots
\ee 
for
\be 
\Lambda^kV=\underbrace{V\wedge\cdots\wedge V}_{k}
\ee 
forms the exterior algebra.
\item Elements of $\Lambda^kV$ are called \emph{k-vectors} (or multivectors in general).
\item In $3d$, the exterior product of two vectors would almost like the cross product of the vectors:
\be 
\left.\begin{aligned}
u=u_1e_1+u_2e_2+u_3e_3\\
v=v_1e_1+v_2e_2+v_3e_3
\end{aligned}\right\}\rightarrow u\wedge v=(u_1v_2-v_2u_1)(e_1\wedge e_2)+\dots
\ee 
Similarly, $u\wedge v\wedge w$ will be almost same as $\vec{u}\.(\vec{v}\x\vec{w})$. These are no coincidences; in fact, we will see that many vector operations can be understood in terms of multivectors.
\item We have seen several area and volume integrations in first year courses: we know that the integrations (such as use of Gauss law in electrodynamics) require \emph{oriented areas}; in practice, we write integrals like $\int f(x,y)dxdy$; however, obviously, the infinitesimal area element $dxdy$ is \emph{not} oriented when written like that. The proper way to write it is actually as $dx\wedge dy$: this is simply a \emph{2-covector}; likewise, the oriented volume element is a \emph{3-covector} $dx\wedge dy\wedge dz$.

\item Hodge duality: Any two vector space with same dimension are isomorphic to each other, i.e. they can be converted to one another. Since $\Lambda^p{V}$ is a vector space with dimensionality $\frac{d!}{p!(d-p)!}$,\footnote{This is the number to choose $p$ elements out of $d$, as the basis vectors of $\Lambda^p(V)$ are ordered combinations (e.g. $e_1\wedge e_2$ and $e_2\wedge e_1$ are not linearly independent).} there is an isomorphism between $p-$vectors and $(d-p)-$vectors: a natural way to achieve this is the \emph{Hodge duality}. If $v$ is a $p-$vector, then its Hodge dual (denoted as $\star v$) is a $(d-p)-$vector!

\item We physicists mostly work only with components (components of the vector, multivector, tensor, etc.) instead of the actual object, so for us it is sufficient to know how to relate the components of a $p-$vector and its Hodge dual.\footnote{
More abstractly, we can understand Hodge duality as follows.
\\\\
Let $V::\texttt{Inner Product Vector Space}$ have the inner product $\<\.,\.\>::(V,V)\to\C$. This inner product \emph{induces} and inner product in the vector space $(\star\a)_{i_{k+1}\dots i_d}$ as follows:
\be
\<\.,\.\>_k::(\Lambda^kV,\Lambda^k V)\to\C
\ee 
for 
\begin{multline}
\<\a_1\wedge\dots\wedge\a_k,\b_1\wedge\dots\wedge\b_k\>=\\\det\begin{pmatrix}
	\<\a_1,\b_1\> &\dots & \<\a_1,\b_k\>
	\\\vdots\\
	\<\a_k,\b_1\> &\dots & \<\a_k,\b_k\>
\end{pmatrix}
\end{multline} 
The Hodge-dual of a $k-$vector $\b$ is then defined in terms of this induced inner product as
\be 
(\forall \a\in \Lambda^k(V))\quad \a\wedge(\star\b)=\<\a,\b\>e_1\wedge\dots \wedge e_n
\ee 
where $e_i$ form the orthonormal basis of $V$.
} In the convention
\begin{subequations}
\label{eq: Hodge dual}
\be 
\a=\a_{i_1\dots i_k}e^{i_1}\wedge\cdots\wedge e^{i_k}\\
\star\a=(\star\a)_{i_{k+1}\dots i_d}e^{i_{k+1}}\wedge\cdots\wedge e^{i_d}
\ee 
the relation reads as
\be 
(\star\a)_{i_{k+1}\dots i_d}=\frac{1}{(d-k)!}\a_{i_1\dots i_k}\e^{i_1\dots i_kl_{k+1}\dots l_d}\de_{i_{k+1}l_{k+1}}\cdots \de_{i_dl_d}
\ee 
\end{subequations}
if we are working in Euclidean spaces, which will be the focus of this course! Here $\de$ is the Kronecker delta symbol.
\item We discussed examples of Hodge dualities in $2d$ and $3d$, e.g.
\be 
\star 1 =dx\wedge dy\;,\quad \star dx=dy\;,\quad \star dy=-dx\text{ in }2d
\ee 
\item We discussed how Hodge duality helps us define the cross product in $3d$: $\star \left(u\wedge v\right)=u\times v$.
\end{enumerate}}

\subsection{Division algebras}
\draftnote{
Following is not covered in class or in homeworks; may be covered next years:\\\\	
Normed division algebras and Hurwitz's theorem; finite-dimensional associative division algebras over the real numbers and Frobenius theorem; more about quaternions, their usage in spatial rotation (hence applications in engineering), and Feza Gursey's work with quaternions}

\chapter{Differentiation in Vector Calculus}
\section{Vector fields and forms on Euclidean Spaces}
\draftnote{
\begin{enumerate}
	\item In class, we discussed 2d and 3d pendulums: at different positions, the velocity at that position belongs to the vector space \emph{at that position}. Even though these vectors spaces are isomorphic to each other,\footnote{Any two finite dimensional vector spaces with the same dimension are actually isomorphic to each other.} i.e. they can be transformed to each other, they are actually different vector spaces! This is not only true for pendulums, but for any motion! 
	\item Let us assume that we are going to describe the kinematics of a particle. We need to specify the position and the velocity of the particle at that position: this is the complete information to understand the trajectory of the particle. In other words, if we know all possible $(r,\vec{v}(r))$ pairs where $r$ parametrizes the particle's position in the system and $\vec{v}(r)$ is the velocity vector in that position, then we can describe the motion completely!
	\\\\
	Consider a two dimensional pendulum. We can parametrize the object's position by an angle $\theta$ as the trajectory forms a circle. At any position, its velocity lies in the \emph{tangent line} at the pendulum's position to this circle: note that these lines are \emph{different} at different values of $\theta$, so we indeed have infinitely many vector spaces, one for each value of $\theta$. We could choose $\pdr{}{\theta}$ as the basis vector; then the relevant pairs would look like $\left(\frac{\pi}{6},2\pdr{}{\theta}\right)$ or $\left(-\frac{\pi}{3},-\pdr{}{\theta}\right)$. The first one says that at $\pi=\pi/6$, the velocity is $2\pdr{}{\theta}$ at the tangent space to the circle at $\theta=\pi/6$. We will see later that this basically means the particle is moving such that at $\theta=\pi/6$, it will move to increase its $\theta$. On the contrary, at $\theta=-\pi/3$, it will move to decrease its $\theta$ (as the velocity at that point is $-\pdr{}{\theta}$). If we are to combine all such $(\theta,v(\theta)\pdr{}{\theta})$ pairs, we have the complete behavior of the pendulum!
	\\\\
	To understand these better, we need to introduce some terminology.
	\item We call a space \emph{Manifold} if it resembles the Euclidean space near each point.\footnote{We actually need to know \emph{topological spaces} to define manifolds more appropriately; nevertheless, this definition is good enough for this course.} For instance, $\R^k$ is trivially a Manifold, it is Euclidean space itself. A more nontrivial example is a circle (denoted as $S^1$) or a sphere (denoted as $S^2$): near any point on a circle or a sphere, the space \emph{looks like} a Euclidean space!\footnote{This is why flat Earth believers are so confused: the surface of our planet is a Manifold, and at around any point, we see it approximately as a flat plane $\R^2$.}
	\item At any point on a Manifold $M$, we have the \emph{tangent space} to the manifold denoted as $T_xM$. For instance, $T_xS$ denotes the tangent like to the circle $S$ at the position $x$; likewise, $T_xS^2$ denotes the tangent plane to the sphere $S^2$ at the position $x$. Tangent spaces are necessarily flat, i.e. they are simply $\R^d$ where $d$ is the dimension of the Manifold.
	\item Clearly, $T_xM$ and $M$ are different spaces: this is most easy when the Manifold is not $\R^d$. For instance, if we consider a pendulum, the position of the object is an element of the Manifold $S$, and its velocity at any point $x$ is an element of the tangent space $T_xS$. Clearly, the tangent space (simply the tangent line $\R$ in this case) is a linear space, hence its elements are vectors (called tangent vector), which makes sense as we expect the velocity to be a vector! 
	\\\\
	In other examples, it might look tricky. For instance, for a car moving on a plane, the manifold is $\R^2$, and the tangent space at any point is also $\R^2$: even though these two spaces are \emph{isomorphic}, they are actually different: position is an element of the manifold $\R^2$, whereas the velocity at any position is an element of the tangent space $\R^2$ at that position.
	
	\item Consider a manifold $M$ and an element $x\in M$. Clearly we have infinitely many tangent spaces $T_xM$ for all $x\in M$. We can then define so-called \emph{tangent bundle} of this manifold as the collection of all such vector spaces! Tangent bundle of the manifold $M$ is denoted as $TM$ and we will define it\footnote{\label{footnote: tangent bundle}A more approriate definition of tangent bundle would involve \emph{disjoint union} of individual tangent spaces. As there is no unique way to define disjoint unions (they are only defined up to isomorphisms), we can in fact define $TM$ in different ways as well. We will not be using the explicit definition of $TM$ in this course, so our definition is correct and good enough!} as 
	\be 
	\label{eq: bundle}
	TM=\left\{(x,y)\;|\;x\in M,\; y\in T_xM\right\}
	\ee 
	\item At any point on the Manifold, we can also define the dual vector space $T_x^*M$, which is the vector space whose elements are linear functions from $T_xM$ to the underlying field. $T_x^*M$ is called \emph{cotangent space}, and by combining all of them for a given Manifold $M$, we obtain the \emph{cotangent bundle} denoted as $T^*M$.
	\item In physics, we tentatively call a function \emph{field} if its domain is the Manifold.\footnote{Note that this has nothing to do with the mathematical term \emph{field}, which refers to a set with two operations satisfying certain conditions as detailed in \S~\ref{}.} For instance, the function $f::M\to \R$ is a real scalar field on the manifold $M$; for instance, \emph{the temperature field} is a function which takes an element of the manifold (the position in the space) and spits out a real number, i.e. the temperature at that position. Likewise the \emph{electric field} is a function which takes an element of the manifold as input and spits out a vector, i.e. the electric field vector at that position.
	
	\item In the partial derivative notation of vectors (see 	\eqref{eq: vector derivative notation} for an example), \emph{the vector fields} become a simple generalization of that; for instance,
	\be 
	V::{}&{}\R^3\to T\R^3\\
	V={}&{}(x,y,z)\to\Big(x^2\pdr{}{x}+xz\pdr{}{y}+\frac{1}{x+y}\pdr{}{z}\Big)
	\ee 
	is a $3d$ vector field.\footnote{We are being very schematic with our definitions and types of objects when it comes to bundles. Indeed, the way we defined in \eqref{eq: bundle}, $V$ can have the codomain $T\R^3$ only if its output is actually $\left((x,y,z)\;,\;\Big(x^2\pdr{}{x}+xz\pdr{}{y}+\frac{1}{x+y}\pdr{}{z}\Big)\right)$. In fact, even this is not true: $(x,y,z)$ does not really represent the element of $M$ but rather an element of a \emph{chart} on $M$, which we do not cover in these notes. Furthermore, as we stated earlier in footnote~(\ref{footnote: tangent bundle}), the tangent bundle is defined only upto isomorphisms, hence even with charts introduced, we need to be extra careful with our definitions if we want rigor. Instead, we will only use types as \emph{schematic reminders} in this chapter and do not pursue the topic any further.
	}
	
	\item Multivector fields on the cotangent space, or equivalently covector fields on the tangent space, are called \emph{differential forms} (forms for short). For instance,
	\be 
w::{}&{}\R^3\to \Lambda^2(T_x\R^3)\\
w={}&{}(x,y,z)\to\Big(x^2dx\wedge dy+xzdx\wedge dz+\frac{1}{x+y}dy\wedge dz\Big)
\ee 
is a 2-form on the manifold $\R^3$.

	\item Tensor fields are also straightforward generalizations of tensors to mappings from manifolds; for example,
	\be 
	T::{}&{}S^2\to (T_x\R^2)\otimes(T_x\R^2)\otimes(T_x^*\R^2)\\
	T={}&{}(\f,\psi)\to\left(\sin(\psi)\pdr{}{\f}\otimes\pdr{}{\f}\otimes d\f+\cos(\psi+\f)\pdr{}{\psi}\otimes\pdr{}{\f}\otimes d\psi\right)
	\ee 
	is a type (2,1) tensor field on the manifold $S^2$.
\item Musical isomorphism: We define two operations that convert back and forth between tangent and cotangent bundles:
\bea 
\Flat ::{}&{} TM\to T^*M\\
\Sharp ::{}&{} T^*M\to TM
\eea 
which in practice amounts to raising and lowering the indices on the components of the tensors, i.e.
\be 
(x^ie_i)^\Flat=(x_i e^i)\;,\quad (x_ie^i)^\Sharp=(x^i e_i)
\ee 
In practice, we physicists do not use musical isomorphism terminology: the manifolds we work with are usually something called \emph{a metric space}, meaning that they are endowed with a \emph{metric} which is type $(0,2)$ tensor field (and the inverse metric which is type $(2,0)$ tensor field). The metric is then used to go back and forth between the dual spaces. In this course, we stick to Euclidean spaces, hence we do not need to introduce the metric tensor and its properties, and hence we won't: the discussion of musical isomorphism is simpler and sufficient enough!\footnote{The musical isomorphism is actually broader in the sense that it represent the abstract mapping between the bundles, which may not be actuated with a metric. For instance, \emph{fiber derivative} also induces a map from tangent to cotangent bundle, which is rather relevant in the study of \emph{Legendre transformations}.}
\item The application of musical isomorphism can be extended to $k-$vectors and forms as well, e.g. $(x_{ij}e^i\wedge e^j)^\Sharp=(x^{ij}e_i\wedge e_j)$
\end{enumerate}
}
\section{Grad, div, curl, and Laplacian}
\subsection{Exterior derivative and hodge star operator}
\draftnote{
\begin{enumerate}
	\item We define \emph{exterior derivative} as a map from $p-$form to a $(p+1)-$form, and denote it as $d\w$ where $\w$ is the $p-$form. With the differentials as the basis vectors of the cotangent bundle, it explicitly reads as 
	\be 
	\w=&\w^{i_1\dots i_p}dx_{i_1}\wedge\dots\wedge dx_{i_p}\\
	d\w=&\frac{\partial \w^{i_1\dots i_p}}{\partial x_k}dx_k\wedge dx_{i_1}\wedge\dots\wedge dx_{i_p}
	\ee 
	\item We have discussed several examples of exterior derivative in class. For instance, for the electric potential $V(x,y)=x^2+y^2$ given in a $2d$ plane, the exterior derivative of it reads as $dV=2xdx+2ydy$: if we were to plot them, they are actually vectors radially outward, precisely orthogonal to the equipotential lines (i.e. $V(x,y)=$constant); in fact, the electric field is actually proportional to this exterior derivative, i.e. $\bm{E}\sim dV$.
	\item The importance of the exterior derivative is that it is \emph{basis-independent}. Physically, this means that exterior derivative of an observer-covariant quantity is itself observer-covariant, which is precisely what we need! In contrast, a vector field defined as $v=\frac{\partial f}{\partial x}dx-\frac{\partial f}{\partial y}dy$ is \emph{not} observer-covariant even if the function $f$ is so. In other words, it cannot represent something physical!\footnote{Note to myself: we discussed these points in detail in class, so I better include a nice discussion here as well, along with some external references.}
	\item The Hodge duality introduced for multivectors naturally generalizes to the forms; in fact, we can immediately use the same formula given in \equref{eq: Hodge dual} for forms as follows:
	\begin{subequations}
		\be 
		\a(x_1,\dots,x_d)=\a_{i_1\dots i_k}(x_1,\dots,x_d)e^{i_1}\wedge\cdots\wedge e^{i_k}\\
		(\star\a)(x_1,\dots,x_d)=(\star\a)_{i_{k+1}\dots i_d}(x_1,\dots,x_d)e^{i_{k+1}}\wedge\cdots\wedge e^{i_d}
		\ee 
		and
		\be 
		(\star\a)_{i_{k+1}\dots i_d}(x_1,\dots,x_d)=\frac{1}{(d-k)!}\a_{i_1\dots i_k}(x_1,\dots,x_d)\e^{i_1\dots i_kl_{k+1}\dots l_d}\de_{i_{k+1}l_{k+1}}\cdots \de_{i_dl_d}
		\ee 
	\end{subequations}
	\item Schematically speaking, \emph{musical isomorphism}, \emph{Hodge-duality}, and \emph{exterior derivative} are our necessary and sufficient tools to construct \emph{differential maps} from scalar \& vector fields to scalar \& vector fields. In the next section, we will then use these to define various vector differentiation operations.
\end{enumerate}
}

\subsection{Invariant formulation of differentiation in vector calculus}
\draftnote{
\begin{enumerate}
	\item We define the gradient (denoted as \textsf{grad}) as follows:
	\be 
	\textsf{grad} ::{}&{} \texttt{Scalar Field}\to\texttt{Vector Field}\\
	\textsf{grad}= {}&{}f\to (df)^\Sharp\\
	\textsf{grad}= {}&{}\big((x_1,\dots,x_d)\to f(x_1,\dots,x_d)\big)\\
	&\quad\to \left((x_1,\dots,x_d)\to \pdr{f(x_1,\dots,x_d)}{ x_i}\hat{x_i}\right)\quad(\text{in Cartesian coordinates})
	\ee 
	\item It is also commonly denoted as $\grad$; for instance, for the scalar field $f(x,y)=x^2+y$, we write the vector field as $\grad{f}(x,y)=2x\pdr{}{x}+\pdr{}{y}$. In the freshman notation, this simply is $\grad{f}(x,y)=2x\hat{i}+\hat{j}$. 
	
	\item It is easiest to understand the gradient visually: For any scalar field $f$ whose value at any point gives the scalar $S$ (temperature, electric potential, pressure, etc.), draw the equi-$S$ lines (equitemperature, equipotential, etc.) which correspond to $S=$constant lines. The vector field $\grad{f}$ then is the collection of vectors that are \emph{orthogonal} to these equi-$S$ lines. In the case of electric potential scalar field $f$, $\grad{f}$ is proportional to the electric field; likewise, in the case of temperature field $f$, $\grad{f}$ is related to the heat flow vector field.
	\item We define the divergence (denoted as \textsf{div}) as follows:
	\be 
	\textsf{div} ::{}&{} \texttt{Vector Field}\to\texttt{Scalar Field}\\
	\textsf{div}= {}&{}v\to \big(\star d\star v^\Flat\big)\\
	\textsf{div}= {}&{}\big((x_1,\dots,x_d)\to v^i(x_1,\dots,x_d)\hat{x_i}\big)\\
	&\quad\to \left((x_1,\dots,x_d)\to \pdr{v_i(x_1,\dots,x_d)}{ x_i}\right)\quad(\text{in Cartesian coordinates\footnotemark})
	\ee 
	\footnotetext{
	It may look counter-intuitive to see that such a simple definition in Cartesian derivative involve so many operations in its abstract definition. Let us do the explicit derivation to see that it makes sense:
	\be 
	v=&v^i(x_1,\dots,x_d)\pdr{}{x^i}\\
	v^\Flat=&v_i(x_1,\dots,x_d) dx^i\\
	\star v^\Flat=&\frac{1}{(d-1)!}v_i(x_1,\dots,x_d)\e^{ik_2\dots k_d}\de_{k_2l_2}\cdots\de_{k_dl_d}\\&\quad\x dx^{l_2}\wedge \dots\wedge dx^{l_d}\\
	d\star v^\Flat=&\frac{1}{(d-1)!}\frac{\partial v_i(x_1,\dots,x_d)}{\partial x^m}\e^{ik_2\dots k_d}\de_{k_2l_2}\cdots\de_{k_dl_d}\\&\quad\x dx^m\wedge dx^{l_2}\wedge \dots\wedge dx^{l_d}\\
	\star d\star v^\Flat=&\frac{1}{(d-1)!}\frac{\partial v_i(x_1,\dots,x_d)}{\partial x^m}\e^{ik_2\dots k_d}\de_{k_2l_2}\cdots\de_{k_dl_d}\e^{ml_2\dots l_d}\\
	\star d\star v^\Flat=&\frac{\partial v_i(x_1,\dots,x_d)}{\partial x^m}\de^{im}\\
	\star d\star v^\Flat=&\frac{\partial v_i(x_1,\dots,x_d)}{\partial x^i}
	\ee 
	where we used $\e^{ia_1\dots a_n}	\e_{ka_1\dots a_n}=n!\de^i_k$.\\
}
	\item It is also commonly denoted as $\div{}$; for instance, for the vector field $v(x,y)=xy\pdr{}{x}-y^2\pdr{}{y}$, we write the scalar field as \mbox{$\div v(x,y)=-y$}.\footnote{This notation is no coincidence: the divergence can be seen as the dot product between the vector field and the gradient operator $\grad$.}
	\item Divergence measures \emph{the scalar source} of the vector field: we will learn more about this when we cover the Helmholtz decomposition.
	\item We define the curl (denoted as \textsf{curl}) as follows:
	\be 
	\textsf{curl} ::{}&{} \texttt{Vector Field}\to\texttt{Scalar Field}\quad (d=2)\\
	\textsf{curl} ::{}&{} \texttt{Vector Field}\to\texttt{Vector Field}\quad (d=3)\\
	\textsf{curl} ::{}&{} \texttt{Vector Field}\to(d-2)-\texttt{Vector Field}\quad (d>3)\\
	\textsf{curl}= {}&{}v\to \big(\star d v^\Flat\big)^\Sharp
	\ee 
	\item The curl operator reduces to the cross product with the gradient operation in $3d$, i.e. $\textsf{curl } v=\curl{v}$; in Cartesian coordinates, it then yields
	\be 
	\curl{v}=\left(\pdr{v_z}{y}-\pdr{v_y}{z}\right)\hat{x}+\left(\pdr{v_x}{z}-\pdr{v_z}{x}\right)\hat{y}+\left(\pdr{v_y}{x}-\pdr{v_x}{y}\right)\hat{z}
	\ee 
	\item Curl measures \emph{the vector source} of the given vector field: we will understand this better with the Helmholtz decomposition.
	\item Discussed the psudovector nature of curl and did a few examples in class.
	\item We now turn to our last operator: the Laplacian (denoted as $\Delta$). In principle, it can take any scalar or multivector field to itself; however, we will only define it for scalar and vector fields:\footnote{Note to myself: maybe discuss Laplace beltrami operator, or at least a few citations?}
	\be 
	\Delta ::{}&{} \texttt{Scalar Field}\to\texttt{Scalar Field}\\
	\Delta= {}&{}f\to \big(\star d\star d f\big)\\
	\Delta ::{}&{} \texttt{Vector Field}\to\texttt{Vector Field}\\
	\Delta= {}&{}v\to \big(d\star d\star v^\Flat-\star d\star d v^\Flat\big)^\Sharp
	\ee 
We only need this abstract definition to ensure that this operator is observer-covariant, i.e. we can do science with it!\footnote{Note to myself: Again, I discussed this whole observer-independence in class, but these notes lack a proper discussion.} In practice, we can also define this same operator as simply \emph{the gradient of divergence} and denote it as $\Delta=\div{\grad}=\grad^2$, which can take any tensor field to another tensor field: in Cartesian coordinates, it simply reads as follows:
\be 
R::{}&{}TM\otimes \dots\otimes TM\otimes T^*M\otimes \dots\otimes T^*M\\
R={}&{}R^{i_1\dots i_r}_{\quad\;\; k_1\dots k_s}\pdr{}{x^{i_1}}\otimes\dots\otimes\pdr{}{x^{i_r}}\otimes dx^{k_1}\otimes\dots\otimes dx^{k_s}\\
(\Delta R)::{}&{}TM\otimes \dots\otimes TM\otimes T^*M\otimes \dots\otimes T^*M\\
\Delta R={}&{}\frac{\partial^2 R^{i_1\dots i_r}_{\quad\;\; k_1\dots k_s}}{\partial x^m \partial x_m}\pdr{}{x^{i_1}}\otimes\dots\otimes\pdr{}{x^{i_r}}\otimes dx^{k_1}\otimes\dots\otimes dx^{k_s}
\ee 
\item In class, derived several identities such as $\div{\curl v}=0$ using their abstract definitions and the identity $d^2=0$.
\item In class, we discussed why we use these generalized definitions even though Cartesian component based ones are actually far more simple! The key point is that this way the result is guaranteed to be observer covariant/invariant! To emphasize, we defined a new operator in class (call it $\cD$) which acts on $2d-$vectors as
\be 
\cD\.A=\pdr{A_x}{y}+\pdr{A_y}{x}
\ee 
Even though this operator does generate a scalar field from a vector field, and even though there is nothing wrong with it mathematically, it is actually observer-dependent. In other words, the scalar that it generates changes when we change the coordinate system. Physically, we do not want scalars to be observer dependent (the temperature at a point should not change when I rotate clockwise 90 degrees). In this example, it is easy to see this as we can straightforwardly check it; but in general, it can become quite tedious for longer operators; and it does not instructs us how we should rather construct our operator. When we work with forms, we are guaranteed that any operator out of exterior derivative, hodge duality, and musical isomorphism is observer independent as it does not refer to a particular basis but is instead defined abstractly.
\end{enumerate}
}

\section{Helmholtz decomposition of vector fields}
\draftnote{
\begin{enumerate}
	\item Any $3d$ vector field can be written as 
	\be 
	E=\text{constant }-\grad \Phi +\curl V
	\ee 
	In most situations, the constant piece is simply zero (so that the behavior of the vector field around infinity is zero) for which we can write
	\be 
	\Phi(r)=\frac{1}{4\pi}\int\limits_{\text{manifold}}\frac{\grad'\.E(r')}{\abs{r-r'}}dV'-\frac{1}{4\pi}\oint\limits_{\text{boundary}}\frac{\hat{n}'\.E(r')}{\abs{r-r'}}dS'
	\ee 
	and 
	\be 
	V(r)=\frac{1}{4\pi}\int\limits_{\text{manifold}}\frac{\grad'\x E(r')}{\abs{r-r'}}dV'-\frac{1}{4\pi}\oint\limits_{\text{boundary}}\frac{\hat{n}'\x E(r')}{\abs{r-r'}}dS'
	\ee 
	
	Here, $\Phi$ is called the \emph{scalar potential} of $E$ whereas $V$ is called the \emph{vector potential} of $E$.
	\item Even if the vector field $E$ is fixed, the vector potentials are not: the vector field is invariant under the change
	\be 
	\Phi\to\Phi+c\;,\quad V\to V+\grad a
	\ee 
	for a constant $c$ and scalar field $a$. This transformation is called \emph{gauge transformation}, and invariance of $E$ under this transformation is called \emph{gauge invariance}.
	\item In class, we discussed the conservative forces (for which vector potential is zero), importance of these \emph{sources} in understanding electromagnetism, and how gauge transformation is actually rather important in quantum mechanics.
	\item In class, we analyzed the vector fields through their flow diagrams: I draw a diagram and we have seem around which parts we have divergence contributions (basically Gauss law), and which parts have rotational contributions (e.g. Ampere's law).
	\item I did not discuss or even mention the \emph{Hodge decomposition}, the generalization of this. Note to myself: decide whether to include a brief mention of this or not next year!
\end{enumerate}
}

\chapter{Integration in Vector Calculus}
\section{Higher Dimensional Integration}
\draftnote{
\begin{enumerate}
	\item Discussed in class that the students have actually learned three kinds of integration so far: indefinite, signed definite, and unsigned definite. The fundamental theorem of calculus relates these three to each other:
	\bea 
	\int f(x)dx=F(x)\quad(\text{indefinite integral})\\
	\int\limits_a^bf(x)=F(b)-F(a)\quad(\text{signed definite integral})\\
	\int\limits_{[a,b]}f(x)dx=\left\{\begin{aligned}
	\int\limits_a^bf(x)dx\quad&b>a\\
	\int\limits_b^af(x)dx\quad&a>b
	\end{aligned}\right.(\text{unsigned definite integral})
	\eea 
	\item When we generalize to higher dimensional integrals, these three actually diverge to different kinds of integrations:\footnote{In other words, fundamental theorem of calculus fails for higher dimensional integrations. The branch of math that analyzes how much it fails is called \emph{de Rham cohomology}.}
	\bea 
	\text{indefinite}\quad&\to\quad\text{anti-derivative}\\
	\text{signed definite}\quad&\to\quad\text{integration of forms}\\
	\text{unsigned indefinite}\quad&\to\quad\text{Lebesgue integration}
	\eea 
	Anti-derivation is useful for solving the partial differential equations, and Lebesgue integrations are used for integrations on measures. The integration of forms on the other hand are the most common types in the elementary Physics, so we will focus on that in these notes.\footnote{Note to myself: add examples for all.}
	\item In class, we discussed how one-dimensional ordinary integration can be seen as summation of covectors. I'll write these in more detail with figures in future (hopefully next year). Interested reader can consult to Terence Tao's ``Differential Forms and Integration'' notes in \emph{The Princeton Companion to Mathematics}.
	\item In class, we discussed how ambient space and integration surfaces can be of different dimensions, and how signed definite integration (which generalizes to integration of forms) is actually pairing of a $p-$form with a $p-$dimensional integration surface to create a scalar. For instance, in Gauss'law, we integrate a $2-$form (electric flux) against a $2-$dimensional integration surface (a closed Gaussian surface) which yields a scalar (total enclosed charge). Likewise, the infinitesimal work done can be seen as a $1-$form and by integrating it against a path ($1-$dimensional surface), we get a real number (the work done).
	\item The simplest generalization of one-dimensional signed definite integral is the line integral in higher dimensions (for which integration surface is a one-dimensional line in a higher dimensional ambient space), which we will focus next. But for these integrals, we need to  understand how we can mathematically describe paths, which we turn to next.
\end{enumerate}
}
\section{Curves and Line Integrals}
\draftnote{
\begin{enumerate}
	\item Concepts: Parametrization, arc length, concept of curvature and torsion, and FrenetSerret formulas
	\item In class, we discussed the above concepts and schematically derived the Frenet-Serret formulas. For more on these topics, see David Tong's lecture notes on vector calculus: \hyperref{http://www.damtp.cam.ac.uk/user/tong/vc.html}{}{}{http://www.damtp.cam.ac.uk/user/tong/vc.html} (see chapter 1).
\end{enumerate}
}
\section{Integral theorems}
\subsection{Gradient, Divergence, Curl, and Greens theorems}
\draftnote{
	\begin{enumerate}
		\item In class, we discussed these theorems and their applications in Physics, see \hyperref{http://www.damtp.cam.ac.uk/user/tong/vc.html}{}{}{http://www.damtp.cam.ac.uk/user/tong/vc.html} (chapter 4) or chapter 6.9-6.11 of the textbook for details.
	\end{enumerate}
}
\subsection{Generalized Stokes theorem: one equation to derive them all!}
\draftnote{
\begin{enumerate}
	\item Generalized Stoke's theorem reads as
	\be 
	\int\limits_{\partial \Omega} \omega=\int\limits_{\Omega}d\omega
	\ee 
	which is an extremely simple yet immensely powerful relation: all integral theorems can be derived using this relation.
	\item In the above equation, $\Omega$ denotes any $(p+1)-$dimensional integration surface, $\partial\Omega$ its $p-$dimensional boundary, $\omega$ a $p-$dimensional differential form, and $d$ the exterior derivative.
	\item The antisymmetric nature of forms ensures that $d^2=0$; this then leads to 
	\be 
	\int\limits_{\partial^2\Omega}\w=\int\limits_{\partial\Omega}d\w=\int\limits_{\Omega}d^2\omega=0
	\ee 
	for any form $\w$ of region $\Omega$. This is then only possible if $\partial^2\Omega=\{\}$, which basically means \emph{the boundary of a region does not have a boundary itself}. We illustrated this in class as it also intuitively makes sense.
	\item We derived a few of integral theorems from the Generalized Stokes theorem. For instance, consider the integral of the exterior derivative of a $0-form$ over a path $\g$. By generalized Stoke's theorem, we have
	\be 
	\int\limits_{\gamma}d f(x_1,\dots,x_n)=\int\limits_{\partial\gamma} f(x_1,\dots,x_n)=f(x_1,\dots,x_n)\evaluated_{\g_i}^{\g_f}
	\ee 
	where the boundary of $\gamma$ is just its end points, hence the integration just becomes the difference of the function evaluated at the final $\g_f$ and the initial $\g_i$ points. As we also have 
\be 
d f(x_1,\dots,x_n)=\frac{\partial f}{\partial x_i} dx_i=\grad f \. {dx}
\ee 
we get the Gradient theorem. Similarly, the exterior derivative of a $1-form$
\be 
E(x_1,\dots, x_n)=E_1(x_1,\dots, x_n)dx_1+\dots E_n(x_1,\dots, x_n)dx_n=\vec{E}\.\vec{dx}
\ee 
is
\be 
dE(x_1,\dots, x_n)=\left(\frac{\partial E_2}{\partial x_1}-\frac{\partial E_1}{\partial x_2}\right)dx^1\wedge dx^2+\cdots =\left(\curl{E}\right)\.\hat{n}dA
\ee 
leading to the curl theorem
\end{enumerate}
}
\chapter{Curvilinear Coordinate Transformations}
Consider the vector field 
\be 
M(x,y)=x\pdr{}{y}-y\pdr{}{x}
\ee 
which is a map from the manifold to the tangent bundle, i.e. its output is a vector at the tangent space to the manifold at the input position. We can draw this vector field at the Cartesian plane; for instance, at $(x,y)=(1,0)$, the vector is $M(1,0)=\pdr{}{y}$, which is a unit vector pointing ``north''. Likewise, at $(x,y)=(0,2)$, $M(0,2)=-2\pdr{}{x}$ which is a vector of length 2 pointing ``west''. If we draw all such vectors in the Cartesian plane, we get the picture that it describes a \emph{counterclockwise rotation} around the origin.

Intuitively, rotation on a plane should be describable with a single parameter, the rotation angle $\theta$. Indeed, if we were to start at any point in the coordinate plane and follow the vector field alongside the vectors, we will observe a simple rotation, describable by the rotation angle. It thus makes sense to switch to a new coordinate system where the vector field was manifestly one-dimensional, i.e. something like $\a\pdr{}{\b}$.

Consider two new parameters $r$ and $\theta$ as functions of the old Cartesian coordinates:
\be
\begin{aligned}
r=&f(x,y)\\
\theta=&g(x,y)
\end{aligned}\quad,\quad
\begin{aligned}
	x=&h(r,\theta)\\
	y=&j(r,\theta)
\end{aligned}
\ee
The vector field then becomes via the chain rule
\be 
M(x,y)=x\left(\pdr{f(x,y)}{y}\pdr{}{r}+\pdr{g(x,y)}{y}\pdr{}{\theta}\right)-y\left(\pdr{f(x,y)}{x}\pdr{}{r}+\pdr{g(x,y)}{x}\pdr{}{\theta}\right)
\ee 
hence
\be 
M(t,\theta)=\a(r,\theta)\pdr{}{r}+\b(r,\theta)\pdr{}{\theta}
\ee 
for 
\bea
\a(r,\theta)=\left[
x\pdr{f(x,y)}{y}-y\pdr{f(x,y)}{x}
\right]_{\substack{	x=h(r,\theta)\\y=j(r,\theta)}}
\\
\b(r,\theta)=\left[
x\pdr{g(x,y)}{y}-y\pdr{g(x,y)}{x}
\right]_{\substack{	x=h(r,\theta)\\y=j(r,\theta)}}
\eea
Let us now choose $f,g$ such that $\a(r,\theta)=0$, i.e.
\be 
\label{eq: polar coordinates}
\frac{1}{y}\pdr{f(x,y)}{y}=\frac{1}{x}\pdr{f(x,y)}{x}
\ee  
Note that $f(x,y)$ is function with two variables (2 degrees of freedom), and above equation puts 1 constraint: we expect the most general solution to be a \emph{function} of 1 variable. In fact, we can easily check (for instance using Mathematica, or just by inspection) that the most general solution to this differential equation is 
\be 
f(x,y)=c(x^2+y^2)
\ee 
for the arbitrary function $c$. This then leads to the condition
\be 
r=c(h(r,\theta)^2+j(r,\theta)^2)
\ee 
which can be satisfied if we choose
\be 
h(r,\theta)&=\sqrt{c^{-1}(r)}\cos(\theta)\\
j(r,\theta)&=\sqrt{c^{-1}(r)}\sin(\theta)\\
c^{-1}(r)&>0
\ee 
Thus, we arrive at the unique parametrization
\be
\begin{aligned}
	r=&c(x^2+y^2)\\
	\theta=&\tan^{-1}\left(\frac{y}{x}\right)
\end{aligned}\quad,\quad
\begin{aligned}
	x=&\sqrt{c^{-1}(r)}\cos(\theta)\\
	y=&\sqrt{c^{-1}(r)}\sin(\theta)
\end{aligned}
\ee
for an arbitrary function $c^{-1}(r)>0$, through which 
\be 
M(x,y)=x\pdr{}{y}-y\pdr{}{x}
\ee 
becomes
\be 
M(r,\theta)=\frac{\partial}{\partial\theta}
\ee 
We usually choose $c^{-1}(r)=r^2$ (hence $c(r)=\sqrt{r}$), which then leads to the standard polar coordinates.

Let us instead consider another vector field, i.e.
\be 
N(x,y)=x\pdr{}{y}+y\pdr{}{x}
\ee 
We still want to find a parametrization that simplifies this, and all of the arguments above apply all the way upto the differential equation \eqref{eq: polar coordinates} which now becomes
\be 
\frac{1}{y}\pdr{f(x,y)}{y}=-\frac{1}{x}\pdr{f(x,y)}{x}
\ee 
leading to 
\be 
f(x,y)=c(x^2-y^2)
\ee
for which the parametrization turns into
\be
\begin{aligned}
	r=&c(x^2-y^2)\\
	\theta=&\tanh^{-1}\left(\frac{y}{x}\right)
\end{aligned}\quad,\quad
\begin{aligned}
	x=&\sqrt{c^{-1}(r)}\cosh(\theta)\\
	y=&\sqrt{c^{-1}(r)}\sinh(\theta)
\end{aligned}
\ee
for an arbitrary function $c^{-1}(r)>0$, through which 
\be 
N(x,y)=x\pdr{}{y}+y\pdr{}{x}
\ee 
becomes
\be 
N(r,\theta)=\frac{\partial}{\partial\theta}
\ee 

Note that we could have found the correct parametrization for $N(x,y)$ by realizing that $N(x,y)=iM(x,-i y)$ hence we should go to polar coordinates in the $(x,-iy)$ cartesian plane, \emph{not} in $(x,y)$ plane. Nevertheless, we may not always be able to relate one vector field to a known one, hence our generic algebraic way of derivation is the most applicable procedure in general settings.
