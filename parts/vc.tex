\chapter{Definition and fundamentals of Vector Spaces}
\section{Preliminaries: some basic terminology}
\subsection{Primer: groups, fields, and algebras}
The vector spaces are some structures build on fields, which are themselves built on groups. The groups are built on sets. So we start with sets.

Our practical reason to start with sets is that we need to properly understand them to understand vectors, but one can even say that set theory is the basis of all mathematics! Indeed, historically, 20th century Mathematicians did try to build the foundations of math on sets, but Russell's paradox squeezed in and ruined the whole program, at least to the degree it was envisioned back then. Today, even more fundamental approaches (such as category theory or type theory) try to explain whole of math, but this is beyond our scope: sets are good enough to be the foundation of everything we will care in this book, so we will stick with them to prepare the background for vector spaces.

\paragraph{Sets:} A set is a collection of objects, and its elements determine what the set is. Importantly, we know all of the elements of a set!\footnote{This is actually a rather important point for deep mathematical reasons that we will not visit, so what follows in this footnote is beyond the scope of this book (rather disappointing for me).\\\\ The fact that we cannot know everything in a quantum system suggests that we should probably not use sets for quantum information, and this is actually true! The right mathematical object to use in quantum system does not have the type \texttt{Set} but has the type \texttt{Hilb}, or more explicitly, quantum systems belong to a different \emph{category} then the category of sets! An explicit discussion of these would require us to cover category theory, but an interested reader can consult here \draftnote{add sources}.}

Sets can be denoted as explicit list of elements within curly brackets;\footnote{Note that sets are orderless\footnotemark and that duplication of elements do not change a set, e.g.
\bea 
A,B,C::{}&\texttt{Set}\\
A={}&\{1,2,3,4\}\\
B={}&\{1,4,3,2\}\\
C={}&\{1,2,3,4,3,2,1\}
\eea 
implies $A=B=C$.
}
\footnotetext{
Ordered sets are denoted with \emph{round} brackets, e.g.
\bea 
A::{}&\texttt{Ordered Set}\\
A={}&(1,2,3)
\eea 
where $(1,2,3)\ne(1,3,2)$. Ordered sets can be constructed via usual sets, e.g.
\be 
(1,2,3)\leftrightarrow\{\{1\},\{1,2\},\{1,2,3\}\}\\
(1,3,2)\leftrightarrow\{\{1\},\{1,3\},\{1,2,3\}\}\\
\ee 
} for instance
\bea 
A::{}&\texttt{Set}\label{eq: set example type}\\
A={}&\{1,2,3\}\label{eq: set example}
\eea 
Equation~\eqref{eq: set example} shows that the set $A$ consists of the elements $1$, $2$, and $3$. Equation~\eqref{eq: set example type} on the other hand tells us that \emph{$A$ is of type Set}, i.e. $A$ is a set: for more details about this notation, please check out Section~\ref{sec: type notation and functions}.

Sets can have infinitely many elements and we can use dots to infer the rest from the given; for instance, the set $\{\dots,-1,1,3,5,\dots\}$ can denote the set of all odd integers. Although rather convenient, this method is inherently ambiguous; a more proper way to denote sets of infinite elements is through \emph{set comprehensions}.

A set comprehension is a way to build a set, and it is usually used with \emph{predicates}.\footnote{Predicates are functions with the codomain \texttt{Boolean}.\footnotemark In other words, a function is a predicate if it only yields two outputs: \textsf{True}, or \textsf{False}. For instance, the function ``isOddInteger'' defined as
\bea 
\text{isOddInteger}::{}&\texttt{Boolean}\\
\text{isOddInteger}={}&x\to \left(\frac{x+1}{2}\in\Z\right)
\eea 
is a \emph{predicate}, e.g. \mbox{$\text{isOddInteger}(1)=$\textsf{True}}, \mbox{$\text{isOddInteger}(2)=$\textsf{False}}, \mbox{$\text{isOddInteger}(5/2)=$\textsf{False}}, etc.
}
\footnotetext{This is the definition due to \emph{Gottlob Frege}, the father of axiomatic first-order logic. You may find other definitions in different branches of math: an interested reader can check Wikipedia for more.} There are two conventional ways to write down a set $A$ this way:
\begin{itemize}
	\item $A=\{x\in\text{ Domain}\;|\;\text{conditions}\}$
	\item $A=\{f(x)\;|\;\text{conditions}\}$ which is just an abbreviated version of the method above in the sense that this form actually means
\end{itemize}
\be 
A={}&{}\{y\in\text{ Domain inferred from the conditions}\\&\quad|\;\exists x\text{ s.t. }y=f(x)\text{ \& }\text{conditions}\}
\ee 
For instance, the rigorous way to denote the odd integers, compared to a convenient casual notation $\{\dots,-3,-1,1,3,\dots\}$, is\linebreak $\{a\in\Z\;|\;(\exists n\in\Z)\;a=2n+1\}$ by the first approach.\footnote{This would read as \emph{``the set of integer $a$'s for which there exists an integer $n$ such that $a=2n+1$ is true''}.\footnotemark}\footnotetext{
We can understand this more deeply as follows. Consider the \emph{predicate} $P$ defined as 
\be 
P::{}&{}\Z\to\texttt{Boolean}\\
P={}&{}a\to (\exists n\in\Z)\;a=2n+1
\ee 
The set of odd integers are then constructed by using this predicate with the list comprehension, i.e.
\be 
\text{odd integers}=\{a\in\Z\;|\; P(a)\}
\ee 
This also shows how the list comprehension works: $S=\{T\;|\; C\}$ means that the set $S$ consists of all elements of $T$ for which the condition $C$ is true.
}${}^{\;}$\footnote{We remind the reader that $\exists$ reads as \emph{``there exists$\dots$''}, and it is one of the \emph{quantifiers} in logic. The other two important ones that we will make use of are $\forall$ and $\exists !$, which denote \emph{``for all$\dots$''} and \emph{``there exists a unique$\dots$''} respectively. For instance, $(\exists!\;n\in\Z)\;n^2=n$ is the statement \emph{``there exists a unique integer $n$ such that $n^2=n$''}, and it is \textsf{False} as both $0$ and $1$ satisfy $n^2=n$: $\boxed{\left((\exists!\;n\in\Z)\;n^2=n\right)=\textsf{False}}$.\footnotemark\\\\
It is no coincidence that I put the dots to the right of the expressions for the quantifiers; one should read them as in a sentence, hence their order matters. For instance
\bea 
\Big((\forall a\in\Z)(\exists b\in\Z)\;b=2a\Big)=\textsf{True}\\
\Big((\exists b\in\Z)(\forall a\in\Z)\;b=2a\Big)=\textsf{False}
\eea 
where the first expression is \emph{``For all integers, there exists an integer which is twice the former''} and the second one is \emph{``There exists an integer which is twice all integers''}.
}\footnotetext{
It may seem weird at first that there is a variable in the left hand side of the equality $\left((\exists!\;n\in\Z)\;n^2=n\right)=\textsf{False}$, which does not appear on the right hand side. The variable $n$ here is a \emph{dummy/scooping/bound} variable hence the right hand side should be independent of it, just like the function $g(y)$ in $\int f(x,y)dx=g(y)$ being independent of $x$. For more details about this, see the comprehensive footnote~\ref{footnote:dummy variables}.} In comparison, the second approach makes it simpler: $\{2n+1\;|\; n\in\Z\}$.

I will assume that the readers are familiar with the comparisons of sets\footnote{$A\subseteq B$ (or $B\supseteq A$) means that all elements of $A$ are also elements of $B$ ($A$ is called a \emph{subset} of $B$), and $A\subset B$ (or $B\supset A$) means the same thing with the additional condition that $A\ne B$ ($A$ is called a \emph{proper subset} of $B$).} and miscellaneous details of sets.\footnote{For instance, $\abs{S}$ denotes the number of elements in the set $S$, e.g. $\abs{\{1,2,4\}}=3$. If the set $S$ is infinite, we write $\abs{S}=\infty$. As expected, $S\subseteq T$ implies $\abs{S}\le\abs{T}$. I would also like to introduce \emph{power sets}: the power set of a set $S$ is denoted by $\mathcal{P}(S)$ and it is the set whose elements are the subsets of the set $S$. For example,
\bea
S,\mathcal{P}(S)::{}&{}\texttt{Set}\\
S={}&{}\{a,b,1\}\\
\mathcal{P}(S)={}&{}\Big\{\{\},\{a\},\{b\},\{1\},\{a,b\}\\
{}&{}\{a,1\},\{b,1\},\{a,b,1\}\Big\}
\eea 
We note that if the set $S$ is finite, we have $\abs{\cP(S)}=2^{\abs{S}}$, which is indeed the case in the above example ($8=2^3$). This relation is why the power set of $S$ is also loosely denoted as $2^S$.
} I will also skip discussing the details of operations between sets.\footnote{Six important operations to know are \emph{union}, \emph{intersection}, \emph{difference} (also called \emph{complement}), \emph{Cartesian product}, \emph{disjoint union}, and \emph{quotient by an equivalence relation}. I expect the reader to know the first four operations: the last two will not be relevant for this book, but I advice the interested reader to learn more about them.} Lastly, I would like to mention the relevant concepts such as \emph{relations}, \emph{partitions}, and \emph{quotients}: one should know about them for a good general mathematical background, but they will unfortunately not be covered in this book.

\paragraph{Groups:} Consider a set $S$ and impose the existence of two functions $o$ and $i$ such that 
\bea 
S::{}&{}\texttt{Set}\\
o::{}&{}(S,S)\to S\\
i::{}&{}S\to S
\eea 
where $S$ is of type set, and the function $o$ has the domain ``tuple of $S$'' and the codomain $S$ ---check out section~\ref{sec: partial differential equations} for more details about tuples. In simpler terms, the function $o$ takes two inputs (both being elements of the set $S$) and yields an element of the set $S$; in comparison, the function $i$ takes an element of $S$ to another (not necessarily different) element of $S$.

The set $S$ with these two functions $o$ and $i$ form a group if the following statements are true:
\begin{enumerate}
	\item $\boxed{(\exists e\in S)(\forall s\in S)\; o(e,s)=o(s,e)=s}$. In words, this says that\linebreak \emph{``There exists an element $e$ of the set $S$ and $o(e,s)=o(s,e)=e$ for any element $s$ of this set''}. The element $e$ is called \emph{the identity element} with respect to the \emph{group operation} $o$.\footnote{Suppose for a second that there are two identity elements: $e_1,e_2$. Since $e_1$ is identity element, we need $o(e_1,e_2)=e_1$; but the same argument for $e_2$ dictates $o(e_1,e_2)=e_2$: we need $e_1=e_2$. In conclusion, if identity element exists, it is \emph{unique}!}
	\item  $\boxed{(\forall s \in S)\; o(s,i(s))=o(i(s),s)=e}$. In words, this says that \emph{``The group operation $o$ acting on the tuple $(s,i(s))$ or $(i(s),s)$ yields the identity element $e$, for any element $s$ of the set $S$''}. In practice, this condition specifies the action of $i$ on any element $s$, and $i(s)$ is called the \emph{inverse} of the element $s$.
	\item $\boxed{(\forall a,b,c \in S)\; o(a,o(b,c))=o(o(a,b),c)}$. This is called \emph{associative law}, and it is best visualized when we use \emph{infix notation}: if we denote $o(a,b)$ as $a*b$, then this condition simply reads as $a*(b*c)=(a*b)*c$. Therefore, the parentheses are not important, and we can even loosely denote without parentheses: $a*(b*c)=(a*b)*c=a*b*c$.
\end{enumerate}

We will \emph{define} a group to be the pair $(S,o)$, i.e.
\be 
\texttt{Group}=\Big(\texttt{Set},\;(\texttt{Set},\texttt{Set})\to\texttt{Set}\Big)
\ee 
such that the above statements are true (satisfaction of those rules will in turn determine the inverse function $i::S\to S$ as well). For instance, the pair $(\Z,+)$ is a group, where $+$ denotes the arithmetic addition, and the inverse function turns out to be multiplication by $-1$: we say that \emph{integers form a group under arithmetic addition}. We can indeed check that the above statements are true:
\begin{enumerate}
	\item $(\forall a\in \Z)\; a+0=0+a=a$ (addition by $0$ does not change any integer)
	\item $(\forall a\in \Z)\; a+(-a)=(-a)+a=0$ (addition by the inverse element takes an integer to zero)
	\item $(\forall a,b,c \in \Z)\; a+(b+c)=(a+b)+c$ (the order of addition does not change the result)
\end{enumerate}

As another example for groups, consider the set\linebreak$\textsf{classrooms}=\{P1,P2,P3\}$ and the group operation denoted as $<>$ in infix notation. If we are given the information
\be 
\label{ex: classrooms}
P1<>P1={}&{}P3\\
P2<>P2={}&{}P2\\
P3<>P3={}&{}P1\\
P1<>P2=P2<>P1={}&{}P1\\
P1<>P3=P3<>P1={}&{}P2\\
P2<>P3=P3<>P2={}&{}P3
\ee 
we can immediately deduce that $P2$ is the identity element, and $P1^{-1}=P3$, $P2^{-1}=P2$, \& $P3^{-1}=P1$ (inverse operations). One can check that the necessary conditions are satisfied, hence we state
\be 
(\textsf{classrooms},<>)::\texttt{Group}
\ee 
We will leave further details of groups to the interested reader to research! One important exception will be the concept of \emph{commutativity}: if the group operation is commutative, i.e. $a*b=b*a$, then that group is called a \emph{commutative group}. In commutative groups, one conventionally use the infix operator $+$ instead of $*$ for the group operation, $-s$ instead of $s^{-1}$ to denote inverse operation, and $0$ instead of $1$ to denote identity element. For instance, we can rewrite the example in \equref{ex: classrooms} as $\textsf{classrooms}=\{P1,P2,P3\}=\{P1,0,-P1\}$. Note that the minus ($-$) sign and the symbol $(0)$ has nothing to do with the arithmetic negation and the number $0$ in general: this is simply a conventional notation.

\paragraph{Rings:} Consider a set $S$ and two functions $+$ and $\.$ such that 
\bea 
S::{}&{}\texttt{Set}\\
+::{}&{}(S,S)\to S\\
\.::{}&{}(S,S)\to S
\eea 
where we will denote the functions in the infix form, e.g. $a+b$ instead of $+(a,b)$. We say that the triplet $(S,+,\.)$ is a ring, i.e.
\be 
(S,+,\.)::\texttt{Ring}
\ee
if the following statements are true:
\begin{enumerate}
	\item $\boxed{(S,+)\text{ is a commutative group}}$
	\item $\boxed{(\forall a,b,c\in S)\; a\.(b+c)=a\.b+a\.c}$
	\item $\boxed{(\forall a,b,c\in S)\; (b+c)\.a=b\.a+c\.a}$
\end{enumerate}
In other words, rings are simply commutative groups with an additional operation (call multiplication) that distributes over group addition.

The most familiar rings are those where the ring addition and multiplication operations are literally arithmetic addition and multiplication; for instance, $(\Z,+,\.)$ forms a ring and so does $(\R,+,\.)$. Likewise, rectangular matrices with complex entries form a ring under matrix addition and matrix multiplication, i.e. $(\cM_{n\x n}(\C),+,\.)::\texttt{Ring}$.

\paragraph{Skew field:} We say that a triplet $(S,+,\.)$ is a skew field, i.e.
\be 
(S,+,\.)::\texttt{Skew Field}
\ee
if the following statements are true:
\begin{enumerate}
	\item $\boxed{(S,+,\.)\text{ is a ring}}$
	\item $\boxed{(S\backslash\{0\},\.)\text{ is a group}}$
\end{enumerate}
In other words, skew fields are rings where multiplication operation also forms a group if we remove the identity element of the addition operation. The prototypical example of the skew fields is the \emph{quaternions}.\footnote{\draftnote{Put some discussion of quaternions}.}

\paragraph{Fields:}  We say that a triplet $(S,+,\.)$ is a field, i.e.
\be 
(S,+,\.)::\texttt{Field}
\ee
if the following statements are true:
\begin{enumerate}
	\item $\boxed{(S,+,\.)\text{ is a ring}}$
	\item $\boxed{(S\backslash\{0\},\.)\text{ is a commutative group}}$
\end{enumerate}

Fields are ubiquitous and super important for physicists as we use them everwhere! The rational, real, and complex numbers all form fields under ordinary arithmetic addition and multiplication:\footnote{\label{footnote: exotic example for a field}
For a more exotic example, consider the following set:
\be 
A=\Big\{(x\to x+a)\;\Big|\; a\in \R\Big\}
\ee 
for the ordinary arithmetic addition. This is a set of \emph{functions}, i.e. elements of the set are functions; for instance, one of the elements of this set is $x\to x+1$: this is a function that adds one to its input.
\\\\
Let us rewrite this set in a more familiar way. For this, we define the \emph{higher order function} ``\textsf{plus}'':
\be 
\textsf{plus}::{}&{}\R\to(\R\to\R)\\
\textsf{plus}={}&{}a\to\big(x\to(x+a)\big)
\ee 
This higher order function becomes an ordinary function if given an input, i.e. $\textsf{plus}(1)::\R\to\R$, which can then be given one more input that converts it into a real number: $\textsf{plus}(1)(5)=6::\R$. Indeed, this higher order function simply adds its inputs, i.e. $\textsf{plus}(a)(b)=a+b$. In terms of this higher order function, we can write down the set $A$ above as
\be 
A=\Big\{\textsf{plus}(a)\;\Big|\; a\in \R\Big\}
\ee 
Let us now introduce two higher order functions:
\be
\oplus,\otimes ::{}\big((\R\to\R),(\R\to\R)\big)\to (\R\to\R)
\ee
These functions take a pair of functions, and then give another function! Let us use these guys in infix form, e.g. $\textsf{plus}(a) \oplus\textsf{plus}(b)$.
\\\\
If we are given the information that 
\be 
\textsf{plus}(a) \oplus\textsf{plus}(b)=\textsf{plus}(a+b)
\ee 
we can immediately say $(A,\oplus)$ forms a commutative group, with the identity element $\textsf{plus}(0)$. If we are given the further information
\bea 
{}&\big(\textsf{plus}(a) \oplus\textsf{plus}(b)\big) \otimes\textsf{plus}(c)=\nn\\
{}&\big(\textsf{plus}(a)\otimes\textsf{plus}(c)\big) \oplus\big(\textsf{plus}(b)\otimes\textsf{plus}(c)\big)
\\
{}& \textsf{plus}(c)\otimes\big(\textsf{plus}(a) \oplus\textsf{plus}(b)\big)=\nn\\
{}&\big(\textsf{plus}(c)\otimes\textsf{plus}(a)\big) \oplus\big(\textsf{plus}(c)\otimes\textsf{plus}(b)\big)
\eea 
we can then say that $(A,\oplus,\otimes)$ forms a ring! Finally, if we are also given
\be 
\textsf{plus}(a) \otimes\textsf{plus}(b)=\textsf{plus}(a.b)
\ee 
then we can prove that $(A,\oplus,\otimes)$ is actually a field!
}
\be 
(\Q,+,\.)::\texttt{Field}\;,\qquad (\R,+,\.)::\texttt{Field}\;,\qquad (\C,+,\.)::\texttt{Field}
\ee  

\paragraph{Vector space (Linear space):} We \emph{define} \emph{``a vector space over a field $F$''} as the triple $(V,\oplus,\odot)$, i.e.
\bea
V::{}&{}\texttt{Set}\\
\Big(F=(S,+,\.)\Big)::{}&{}\texttt{Field}\\
\oplus::{}&{}(V,V)\to V\\
\odot::{}&{}(S,V)\to V\\
(V,\oplus,\odot)::{}&{}\texttt{Vector Space}
\eea
if the following statements are true
\begin{enumerate}
	\item $\boxed{(V,\oplus)::\text{Commutative Group}}$
	\item $\boxed{(\forall v\in V)\; 1\odot v=v\text{ where $1$ is the identity element of $\.$}}$
	\item $\boxed{(\forall v\in V)(\forall s\in S)\; s\odot v \in V}$
	\item $\boxed{(\forall v\in V)(\forall a,b\in S)\; (a\.b)\odot v=a\odot (b\odot v)}$
	\item $\boxed{(\forall v\in V)(\forall a,b\in S)\; (a+b)\odot v=(a\odot v)\oplus (b \odot v)}$
	\item $\boxed{(\forall v,w\in V)(\forall s\in S)\; s\odot(v\oplus w)=(s\odot v)\oplus (s\odot w)}$
\end{enumerate}
Here, the elements of the underlying field $F$ are called \emph{scalars} ($s\in S$) and the elements of the linear spaces are called \emph{vectors} ($v\in V$). indeed, $V$ above is the set of these vectors, $\oplus$ is the \emph{vector addition} operation, and $\odot$ is the \emph{scalar multiplication} operation. We will see more properties of vectors (such as basis, linear independence, etc.) in the next section.

The standard example of a linear space is this:
\be
\big(
\left(\cM_{n\x m}(S),\oplus\right),\left(S,+,\.\right)
\big)::\text{Linear Space}
\ee 
In words, we say that all $m\x n$ matrices from a field $\ka=\left(S,+,\.\right)$ form a linear space over $\ka$. In this example, matrices \emph{are} vectors of this linear space and the vector addition $\oplus$ is simply defined as the matrix addition. A more familiar example can be given as follows:
\be 
V={}&{}\{a\bx+b\by+c\bz \;|\; a,b,c\in\R\}\\
\left(a_1\bx+b_1\by+c_1\bz\right)\oplus\left(a_1\bx+b_1\by+c_1\bz\right)={}&{}(a_1+a_2)\bx+(b_1+b_2)\by+(c_1+c_2)\bz\\
s\odot\left(a\bx+b\by+c\bz\right)={}&{}(s\.a)\bx+(s\.b)\by+(s\.c)\bz
\ee 
where we defined the set $V$ and the explicit action of the functions $\oplus$ and $\odot$.\footnote{
Let us consider a more nontrivial example. Consider the field $(A,\oplus,\otimes)$ disucssed in footnote~\ref{footnote: exotic example for a field}. We will define a vector space $(W,\square,\odot)$ over this field. Note that, in this example, $\square$ will denote the vector addition whereas $\oplus$ is reserved for the scalar addition in the field $A$.
\\\\
We define the set $W$ as
\be 
W=\{(a,b)\;|\;a,b\in\R\}
\ee  
whereas we choose the vector addition and scalar multiplications as
\be 
(a,b)\square (c,d)=(a+c,\;b+d)
\ee 
and
\be 
\textsf{plus}(a)\odot (b,c)=(a\.b,\;a\.c)
\ee 
With these definitions, we can verify that all conditions for $(W,\square,\odot)$ to be a vector space over the field $A$ are satisfied.
}
\draftnote{
\begin{enumerate}
	\item (Linear algebra$=$linear space with vector multiplication) is introduced and discussed.
	\item (Lie algebras$=$linear algebra with multiplication being antisymmetric) is introduced and discussed.
	\item Made reference to the commutators and discussed how a linear algebra can be turned into a lie algebra via commutators.
	\item Cross products in three dimensional vector space is discussed from this perspective: $3d$ vectors form a lie algebra thanks to the cross product.
	\item Emphasized how vector spaces can be built on an arbitrary set $V$ as long as the necessary conditions are satisfied. For instance, we can choose
	\be 
	V=\Big\{a\pdr{}{x}+b\pdr{}{y}+c\pdr{}{z}\;\Big|\;a,b,c\in\R\Big\}
	\ee 
	With arithmatic addition and multiplication, these indeed form a vector space. In fact, this way of definition is far more useful than $a\bi+b\bj+c\bk$ as we will see later.
\end{enumerate}
}

\subsection{Properties of Linear spaces}
\draftnote{
\begin{enumerate}
	\item Basis for vector spaces is introduced: let $(B\supset V)::\texttt{Set}$. $B$ is called \emph{a basis} of $V$ if following statements are true:
\begin{itemize}
	\item $(\forall k\in\{1,2,\dim B\})(\forall e_1,\dots,e_k\in B)(\forall c_1,\dots,c_k\in S) [c_1=\dots=c_k=0]\lor[c_1e_1+\dots+c_ke_k=0]$
	\item $(\forall v\in V)(\exists ! a_1,\dots,a_{\dim B}\in S) v=a_1e_1+\dots a_{\dim B}e_{\dim B}$
\end{itemize}
The first statement is the \emph{linear independence} of the basis vectors, whereas second statement is the requirement that the basis \emph{spans} the vector space, i.e. any vector can be decomposed in terms of the basis elements. The coefficients $a_i$ are called the \emph{components} of the vector in this basis.
\item If $\dim B<\infty$, it is called the dimension of the vector space!
\item If $\dim B=\infty$, the vector space is called \emph{infinite dimensional}!
\item Using a basis, a vector can be related to a list/array/tuple of scalars:
\be 
\left.\begin{aligned}
	v::&\texttt{Vector}\\
	a_1,\dots,a_n::&\texttt{Scalar}
\end{aligned}\right\}\Rightarrow v=\begin{pmatrix}
	a_1&\dots&a_n
\end{pmatrix}\begin{pmatrix}
	e_1\\\dots\\e_n
\end{pmatrix}
\ee 
For instance, $(3\bi+4\bk)::\texttt{Vector}$ whereas $(3,0,4)::(\texttt{Scalar},\texttt{Scalar},\texttt{Scalar})$; nevertheless, they represent the same information once the proper basis is given: $3\bi+4\bk=\begin{pmatrix}
	3&0&4
\end{pmatrix}\begin{pmatrix}
	\bi\\\bj\\bk
\end{pmatrix}$.
\item If multiple basis exist for a vector space, all have the same dimension and all can be converted to one another (proof omitted).
\item Index notation and Einstein's summation convention is discussed, i.e. $v=v^ie_i$. Note that index placement is important, i.e. $e^i\ne e_i$.
\item One can choose $a_i$ to represent a row or column matrix (and $a^i$ the other one). Note that $M_i^{\;j}$ or $M^i_{\; j}$ represent a square matrix (as they convert a row/column matrix to another row/column matrix), but $M_{ij}$ or $M^{ij}$ are not square matrices.
\end{enumerate}
}

\section{Inner product spaces}
\draftnote{
\begin{enumerate}
\item Normed vector spaces: in general, we do not need to have functions that takes vectors as inputs and spits out scalars as outputs, i.e. no $(\cdot)::(V,V)\to S$ or $\norm{.}::V\to S$ is necessary for the definition of the vector space.
\\\\
If we impose that a function \emph{\textsf{norm}} such that
\be 
\textsf{norm}::&V\to\R \\
\textsf{norm}=& x\to\norm{x}
\ee 
exists where
\begin{itemize}
	\item $(\forall v\in V) [\norm{v}\ne 0]\lor[v=0]$
	\item $(\forall v\in V)(\forall s\in S)\norm{s\odot v}=\abs{s}.\norm{v}$
	\item $(\forall v,w\in V)\norm{v\oplus w}\le\norm{v}+\norm{w}$
\end{itemize}
and where $V$ is a vector space over the field $\R$ or $\C$, then we call $(V,\norm{.})$ a \emph{normed vector space}.
\item With norm, we can define distance between two vectors: $d::(V,V)\to\R$ and $d=(x,y)\to\norm{x-y}$.
\item With norm, we can define angle between two vectors: $\theta::(V,V)\to\R$ and $\displaystyle \theta=(x,y)\to\acos(\frac{\norm{x}^2+\norm{y}^2-\norm{x-y}^2}{2\norm{x}\norm{y}})$.
\item Inner product spaces: If we have a vector space over the field $\R$ or $\C$, and if we impose the existence of the function $\<.,\.\>$ for 
\be 
F\in&\{\R,\C\}\\
V::&\texttt{Vector Space over }F\\
\<.,.\>::&(V,V)\to F
\ee 
where the following are true:
\begin{itemize}
	\item $(\forall v,w\in V)\;\<v,w\>=\<w,v\>^*$ (conjugate symmetry)
	\item $(\forall u,v,w\in V)(\forall a,b\in F)\; \<au+bv,w\>=a\<u,w\>+b\<v,w\>$
	\item $(\forall v\in V\backslash\{0\})\;\<v,v\>>0$
	\item $\<0,0\>=0$
\end{itemize}
\item Inner product vector spaces can immediately be turned to a normed vector space as well as we can  define $\norm{v}=\sqrt{\<v,v\>}$ which is consistent with all norm requirements. In this definition, the angle between two vectors become $\displaystyle\cos\theta=\frac{\Re(\<x,y\>)}{\norm{x}\norm{y}}$.
\end{enumerate}
}
\section{Dual vector spaces}
\draftnote{
\begin{enumerate}
\item Dual vector spaces: Let $V$ be a vector space over a field $F$. If we consider linear functions that take a vector to a scalar, those functions themselves form a vector space: that is called the \emph{dual vector space} $V^*$. So we have 
\bea 
(F=(S,+,.))::&\texttt{Field}\\
V::&\texttt{Set}\\
\oplus::&(V,V)\to V\\
\odot::&(S,V)\to V\\
(V^*=\{V\to S\})::&\texttt{Set}\\
\oplus^*::&(V^*,V^*)\to V^*\\
\odot^*::&(S^*,V^*)\to V^*\\
\eea 
where 
\bea 
(V,\oplus,\odot)=\text{ vector space over }F\\
(V^*,\oplus^*,\odot^*)=\text{ vector space over }F
\eea 
Gave several examples in class, for instance $V=\{a\bi+b\bj\;|\;a,b\in\R\}$ and $V^*=\{(x\bi+y\bj)\to(ax+by)\;|\;a,b\in\R\}$.
\item Conventionally, elements of $V$ are called \emph{vectors} and elements of $V^*$ are called \emph{covectors}.
\item For the vector space given in the notation
\be 
V=\Big\{a\pdr{}{x}+b\pdr{}{y}+c\pdr{}{z}\;\Big|\;a,b,c,\in\R\Big\}
\ee 
the dual vector space is denoted as
\be 
V^*=\big\{adx+bdy+cdz\;\big|\;a,b,c\in\R\}
\ee 
where $dx\left(\pdr{}{x}\right)=1$, $dx\left(\pdr{}{y}\right)=0$, and similarly.

Note that this convention is more than just a notation: the vector spaces indeed transform functions to functions, and integrations are indeed summations of covectors in a loose sense. We will make these much more concrete later.
\item Discussed in detail and derived that if a basis of the vector space $e_i$ and the basis of the dual vector space $e^i$ transform as 
\be 
e_i\to& M_i^{\;k}e_k\\
e^i\to& e^kN_k^{\;\;i}
\ee 
then the components of the basis vector $v^i$ and the components of the covector $w_i$ transform as 
\be 
v^i\to& v^k(M^{-1})_k^{\;\;i}\\
w_i\to& (N^{-1})_i^{\;k}w_k
\ee 
If we want the action of the covector basis on vector basis to remain unchanged, i.e.
\be 
e^i(e_k)\to e^i(e_k)
\ee 
then we need $N=M^{-1}$. With this, we see that
\be 
e_i\to& M_i^{\;k}e_k\\
w_i\to& M_i^{\;k}w_k\\
e^i\to& e^k(M^{-1})_k^{\;\;i}\\
v^i\to& v^k(M^{-1})_k^{\;\;i}
\ee
We see that the array $w_i$ transforms in the same way as the basis vectors of $V$, i.e. it transforms \emph{covariantly}. We then loosely state that $w_i$ is a \emph{covariant vector}!\footnote{Note that $v^i$ ($w_i$) is actually only the components of the (co)vector, hence it is only an array of the underlying field , i.e. $w_i::(S,S,\dots,S)$ and $v^i::(S,S,\dots,S)$. In contrast, the combination $(w_ie^i)::V^*$ is a covector and the combination $(v^ie_i)::V$ is a vector.} In contrast, $v^i$ transforms in an inverse way compared to $e_i$, and hence it is loosely called a \emph{covariant vector}!
\end{enumerate}
}
\draftnote{To be added: topological vector spaces, Hilbert spaces}

\section{Algebras over fields}
\subsection{Tensor algebras}
\draftnote{
\begin{enumerate}
	\item Consider a linear space over a field $F$. We call \emph{``type $(r,s)$ tensor on a vector space $V$''} an element of 
	\be 
	\underbrace{V\otimes V\otimes\cdot\otimes V}_{r}\otimes\underbrace{V^*\otimes V^*\otimes\cdot\otimes V^*}_{s}
	\ee 
	where $\otimes$ is an associative bilinear map, i.e.
	\be 
	&A=\{V,V^*\}\\
	&\otimes::(A,A)\to A\otimes A\\
	&(\forall a,b,c\in A)\;(a\otimes b)\otimes c=a\otimes(b\otimes c)
	\ee
\item We conventionally denote
\be 
T^kV=\underbrace{V\otimes\cdots\otimes V}_{k}
\ee 
Since the bilinear map $\otimes$ takes the product of $v:: T^kV$ and $\w:: T^l V$ to $(v\otimes\w)::T^{k+l}V$, we need the consider the space of all possible $T^kV$ to have an algebra: this is denoted as $T(V)$ (tensor algebra over $V$) and is defined as
\be 
T(V)=\bigoplus\limits_{k=0}^\infty T^kV=F\oplus V\oplus(V\otimes V)\oplus\cdots
\ee 
A similar definition can be made for $T(V^*)$; the product $T(V)\otimes T(V^*)$ is the algebra over vector field that includes all type of tensors (i.e. $(\forall r,s\in\N)\;(r,s)-$tensor).
\item In Physics, we usually work with components. Indeed, consider the following example:
\be 
a::&V\otimes V\otimes V^*\\
a=&a^{ij}_{\;\;k}e_i\otimes e_j\otimes e^k
\ee 
We usually call $a^{ij}_{\;\;k}$ a tensor: in reality, it is only a collection of scalars (underlying field elements of the vector space); nevertheless, once a basis is specified, there is indeed a unique map between $a$ and $a^{ij}_{\;\;k}$ hence this abuse of notation is justified.
\item $\otimes$ provides an \emph{outer} product, i.e. $a\otimes b$ is not part of the types of $a$ or $b$. For instance, if both $a$ and $b$ are of the type $(1,0)$, $a\otimes b$ is of the type $(2,0)$. In general the value $r+s$ for a type $(r,s)$ tensor is called \emph{the rank of the tensor}, and the rank of the tensor $a\otimes b$ is the sum of the ranks of the tensors $a$ and $b$. For instance:
\be 
\label{eqn: tensor example}
a::&V\otimes V\otimes V^*\\
b::&V^*\otimes V^*\\
(c=a\otimes b)::&V\otimes V\otimes V^*\otimes V^*\otimes V^*\\
a=&a^{ij}_{\;\;k}e_i\otimes e_j\otimes e^k\\
b=&b_{lm}e^l\otimes e^m\\
c=&c^{ij}_{\;\;klm}e_i\otimes e_j\otimes e^k\otimes e^l\otimes e^m\\
c^{ij}_{\;\;klm}=&a^{ij}_{\;\;k}b_{lm}
\ee 
\item As stated earlier, we usually work with components in practice (at least in the Physics), so the product of two tensors $a^{i_1\dots i_r}_{\quad\;\; k_1\dots k_s}$ and $b^{l_1\dots l_t}_{\quad\;\; m_1\dots m_u}$ just becomes $c^{i_1\dots i_r l_1\dots l_t}_{\qquad\quad k_1\dots k_s m_1\dots m_u}=a^{i_1\dots i_r}_{\quad\;\; k_1\dots k_s}b^{l_1\dots l_t}_{\quad\;\; m_1\dots m_u}$.
\item There is a way to change the type and rank of a tensor by an operation called \emph{contraction}. What it does is basically act the basis covector on the basis vector; to see this, consider the tensor $c$ in the equation~\ref{eqn: tensor example}. We can \emph{choose} to contract some of its basis elements to reduce its rank, for instance if we choose to act $e^k$ on $e_i$, we get $e^k(e_i)=\de^k_{\;i}$ for the kronecker-delta $\de$. But since $c^{ij}_{\;\;klm}\de^k_{\;i}=c^{ij}_{\;\;ilm}$, we arrive at
\be 
c'::&V\otimes V^*\otimes V^*\\
c'=&c^{ij}_{\;\;ilm} e_j\otimes e^l\otimes e^m
\ee 
If we instead acted $e^k$ on $e_j$, we would get a different tensor $c''$:
\be 
c''::&V\otimes V^*\otimes V^*\\
c''=&c^{ij}_{\;\;jlm} e_i\otimes e^l\otimes e^m
\ee
Or we could act $e^k$ on $e_i$ and $e^l$ on $e_j$: this would give us an entirely different tensor $c'''$:
\be 
c'''::& V^*\\
c'''=&c^{ij}_{\;\;ijm}e^m
\ee
\item Consider the rotational version of Newton's second law, i.e. \emph{``torque is equal to moment of inertial times angular acceleration''}. We conventionally write it as a matrix equation, i.e. $\tau^i=I^i_{\;j}\a^j$. We see that we could write them as follows:
\be 
\tau,\alpha::&V\\
I::&V\otimes V^*\\
(I\otimes \alpha)=& I^i_{\;j}\alpha^k e_i\otimes e_k\otimes e^j\\
\tau=&I^i_{\;j}\a^j e_i
\ee 
Clearly, we got $\tau$ by first taking the outer product of $I$ and $\a$, and then doing a contraction between two of the indices (equivalently, applying the covector $e^j$ on the vector $e_k$).\footnote{
This line of reasoning is valid to a degree for $3d-$vectors, but in higher dimensions, it is not true. Firstly, $\tau$ and $\a$ are not vectors but \emph{bivectors} in general dimension (we'll cover bivectors below), hence $I$ is actually not a matrix (a rank-2 tensor) but a linear map of bivectors to bivectors (hence a rank-4 tensor): $I::V\otimes V\otimes V^*\otimes V^*$.
}
\end{enumerate}
}
\subsection{Exterior and alternating algebras}
\draftnote{\begin{enumerate}
\item Tensor product $\otimes$ is only defined as an associative bilinear mapping. If we define a similar product with the additional property that it is antisymmetric, we get \emph{exterior algebra}.
\item The relevant antisymmetric operator is called \emph{Wedge product} and is denoted as $\wedge$:
\begin{itemize}
	\item $(\forall u,v\in V)\; u\wedge v=-v\wedge u$
	\item $(\forall u,v,w\in V)\; u\wedge(v\wedge w)=(u\wedge v)\wedge w$
\end{itemize}
\item As the product is fully antisymmetric, any wedge product of $k-$many vectors is zero if $k>\dim(V)$, thus:
\be 
\Lambda(V)=\bigoplus\limits_{k=0}^{\dim(V)} \Lambda^kV=F\oplus V\oplus(V\wedge V)\oplus\cdots
\ee 
for
\be 
\Lambda^kV=\underbrace{V\wedge\cdots\wedge V}_{k}
\ee 
forms the exterior product.
\end{enumerate}}
Wedge product and multivectors, oriented area and volume, hodge duality

\subsection{Division algebras}
\draftnote{
Following is not covered in class or in homeworks; may be covered next years:\\\\	
Normed division algebras and Hurwitz's theorem; finite-dimensional associative division algebras over the real numbers and Frobenius theorem; more about quaternions, their usage in spatial rotation (hence applications in engineering), and Feza Gursey's work with quaternions}


\chapter{Differentiation in Vector Calculus}
\section{Vector fields on Euclidean Spaces}
\draftnote{
\begin{enumerate}
	\item In class, we discussed 2d and 3d pendulums: at different positions, the velocity at that position belongs to the vector space \emph{at that position}. Even though these vectors spaces are isomorphic to each other,\footnote{Any two finite dimensional vector spaces with the same dimension are actually isomorphic to each other.} i.e. they can be transformed to each other, they are actually different vector spaces! This is not only true for pendulums, but for any motion! 
	\item Let us assume that we are going to describe the kinematics of a particle. We need to specify the position and the velocity of the particle at that position: this is the complete information to understand the trajectory of the particle. In other words, if we know all possible $(r,\vec{v}(r))$ pairs where $r$ parametrizes the particle's position in the system and $\vec{v}(r)$ is the velocity vector in that position, then we can describe the motion completely!
	\\\\
	Consider a two dimensional pendulum. We can parametrize the object's position by an angle $\theta$ as the trajectory forms a circle. At any position, its velocity lies in the \emph{tangent line} at the pendulum's position to this circle: note that these lines are \emph{different} at different values of $\theta$, so we indeed have infinitely many vector spaces, one for each value of $\theta$. We could choose $\pdr{}{\theta}$ as the basis vector; then the relevant pairs would look like $\left(\frac{\pi}{6},2\pdr{}{\theta}\right)$ or $\left(-\frac{\pi}{3},-\pdr{}{\theta}\right)$. The first one says that at $\pi=\pi/6$, the velocity is $2\pdr{}{\theta}$ at the tangent space to the circle at $\theta=\pi/6$. We will see late that this basically means the particle is moving such that at $\theta=\pi/6$, it will move to increase its $\theta$. On the contrary, at $\theta=-\pi/3$, it will move to decrease its $\theta$ (as the velocity at that point is $-\pdr{}{\theta}$). If we are to combine all such $(\theta,v(\theta)\pdr{}{\theta})$ pairs, we have the complete behavior of the pendulum!
	\\\\
	To understand these better, we need to introduce some terminology.
	\item 
\end{enumerate}
}
Concept of vector fields, use of partial derivatives to describe vector fields, bivector fields ($F_{\mu\nu}$ in electromagnetism being a bivector), tensor fields
\section{Forms and duals of vector fields}
Forms as covectors, tangent and cotangent bundles, musical isomorphism, index notation of tensors, covariant vs contravariant indices, metric tensor field
\section{Grad, div, curl, and Laplacian}
\subsection{Exterior derivative and hodge star operator}
\subsection{Invariant formulation of differentiation in vector calculus}
The standard vector calculus operators in terms of exterior derivative, hodge star operator, and musical isomorphism.

\section{Helmholtz decomposition of vector fields}
Helmholtz decomposition and a brief mention of its generalization Hodge decomposition

\chapter{Integration in Vector Calculus}
\section{Curves and Line Integrals}
Curves, parametrization, arc length, Frenetâ€“Serret formulas, line integral
\section{Generalized approach to integration}
Chains, boundaries of chains, Poincare lemma
\section{Integral theorems}
Discussion of generalized stokes theorem in forms, and its implication as \emph{Gradient theorem}, \emph{Divergence theorem}, \emph{Curl theorem}, and \emph{Green's theorem}.

\chapter{Curvilinear Coordinate Transformations}
Concept of coordinate charts, transformation of tensor fields under coordinate transformations, common orthogonal curvilinear coordinates (polar, cylindrical, spherical)
